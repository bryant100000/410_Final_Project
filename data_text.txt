data_text.txt
D
B

R
R
Type
Text
Size
255 KB (261,154 bytes)
Storage used
0 bytesOwned by illinois.edu
Location
cs410 group project
Owner
Devyn Theis
Modified
Apr 27, 2019 by Ross Schneider
Opened
Apr 27, 2019 by me
Created
Apr 25, 2019
Add a description
Viewers can download
0:00:[SOUND]
0:09:>> This lecture is about Natural Language of Content Analysis. As you see from this picture, this is really the first step to process any text data. Text data are in natural languages. So computers have to understand natural languages to some extent, in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is natural language processing, which is the main technique for processing natural language to obtain understanding.
0:43:The second is the state of the art of NLP which stands for natural language processing.
0:49:Finally we're going to cover the relation between natural language processing and text retrieval. First, what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand.
1:06:Now what do you have to do in order to understand that text? This is basically what computers are facing. So looking at the simple sentence like a dog is chasing a boy on the playground.
1:18:We don't have any problems understanding this sentence. But imagine what the computer would have to do in order to understand it. Well in general, it would have to do the following. First, it would have to know dog is a noun, chasing's a verb, etc. So this is called lexical analysis, or part-of-speech tagging, and we need to figure out the syntactic categories of those words. So that's the first step. After that, we're going to figure out the structure of the sentence. So for example, here it shows that A and the dog would go together to form a noun phrase.
1:55:And we won't have dog and is to go first. And there are some structures that are not just right.
2:04:But this structure shows what we might get if we look at the sentence and try to interpret the sentence. Some words would go together first, and then they will go together with other words.
2:16:So here we show we have noun phrases as intermediate components, and then verbal phrases. Finally we have a sentence. And you get this structure. We need to do something called a semantic analysis, or parsing. And we may have a parser accompanying the program, and that would automatically created this structure. At this point you would know the structure of this sentence, but still you don't know the meaning of the sentence. So we have to go further to semantic analysis. In our mind we usually can map such a sentence to what we already know in our knowledge base. For example, you might imagine a dog that looks like that. There's a boy and there's some activity here. But for a computer would have to use symbols to denote that.
3:00:We'd use a symbol (d1) to denote a dog. And (b)1 can denote a boy and then (p)1 can denote a playground.
3:12:Now there is also a chasing activity that's happening here so we have a relationship chasing that connects all these symbols. So this is how a computer would obtain some understanding of this sentence.
3:25:Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read a text and this is called inference. So for example, if you believe that if someone's being chased and this person might be scared, but with this rule, you can see computers could also infer that this boy maybe scared. So this is some extra knowledge that you'd infer based on some understanding of the text. You can even go further to understand why the person say at this sentence. So this has to do as a use of language. This is called pragmatic analysis. In order to understand the speak actor of a sentence, right? We say something to basically achieve some goal. There's some purpose there. And this has to do with the use of language. In this case the person who said this sentence might be reminding another person to bring back the dog. That could be one possible intent.
4:33:To reach this level of understanding would require all of these steps and a computer would have to go through all these steps in order to completely understand this sentence. Yet we humans have no trouble with understanding that, we instantly would get everything.
4:52:There is a reason for that. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. Computers unfortunately are hard to obtain such understanding. They don't have such a knowledge base. They are still incapable of doing reasoning and uncertainties,
5:14:so that makes natural language processing difficult for computers. But the fundamental reason why natural language processing is difficult for computers is simply because natural language has not been designed for computers. Natural languages are designed for us to communicate. There are other languages designed for computers. For example, programming languages. Those are harder for us, right? So natural languages is designed to make our communication efficient. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to decipher an ambiguous word based on the knowledge or the context. There's no need to demand different words for different meanings. We could overload the same word with different meanings without the problem.
6:10:Because of these reasons this makes every step in natural language of processing difficult for computers, ambiguity is the main difficulty.
6:18:And common sense and reasoning is often required, that's also hard.
6:23:So let me give you some examples of challenges here.
6:27:Consider the word level ambiguity.
6:30:The same word can have different syntactic categories. For example design can be a noun or a verb.
6:39:The word of root may have multiple meanings. So square root in math sense or the root of a plant.
6:46:You might be able to think about it's meanings. There are also syntactical ambiguities. For example, the main topic of this lecture, natural language processing, can actually be interpreted in two ways in terms of the structure. Think for a moment and see if you can figure that out. We usually think of this as processing of natural language, but you could also think of this as do say, language processing is natural.
7:16:So this is an example of synaptic ambiguity. What we have different is structures that can be
7:24:applied to the same sequence of words. Another common example of an ambiguous sentence is the following. A man saw a boy with a telescope. Now in this case the question is, who had a telescope.
7:38:This is called a prepositional phrase attachment ambiguity or PP attachment ambiguity. Now we generally don't have a problem with these ambiguities because we have a lot of background knowledge to help us disambiguate the ambiguity.
7:55:Another example of difficulty is anaphora resolution. So think about the sentence John persuaded Bill to buy a TV for himself. The question here is does himself refer to John or Bill? So again this is something that you have to use some background or the context to figure out. Finally, presupposition is another problem. Consider the sentence, he has quit smoking. Now this obviously implies that he smoked before.
8:22:So imagine a computer wants to understand all these subtle differences and meanings. It would have to use a lot of knowledge to figure that out. It also would have to maintain a large knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. So this is why it's very difficult.
8:45:So as a result, we are steep not perfect, in fact far from perfect in understanding natural language using computers. So this slide sort of gains a simplified view of state of the art technologies.
9:01:We can do part of speech tagging pretty well, so I showed 97% accuracy here. Now this number is obviously based on a certain dataset, so don't take this literally. This just shows that we can do it pretty well. But it's still not perfect. In terms of parsing, we can do partial parsing pretty well. That means we can get noun phrase structures, or verb phrase structure, or some segment of the sentence, and this dude correct them in terms of the structure.
9:34:And in some evaluation results, we have seen above 90% accuracy in terms of partial parsing of sentences. Again, I have to say these numbers are relative to the dataset. In some other datasets, the numbers might be lower. Most of the existing work has been evaluated using news dataset. And so a lot of these numbers are more or less biased toward news data. Think about social media data, the accuracy likely is lower.
10:05:In terms of a semantical analysis, we are far from being able to do a complete understanding of a sentence. But we have some techniques that would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. For example, recognizing dimensions of people, locations, organizations, etc in text. So this is called entity extraction. We may be able to recognize the relations. For example, this person visited that place or this person met that person or this company acquired another company. Such relations can be extracted by using the computer current Natural Language Processing techniques. They're not perfect but they can do well for some entities. Some entities are harder than others.
11:03:We can also do word sense disintegration to some extend. We have to figure out whether this word in this sentence would have certain meaning in another context the computer could figure out, it has a different meaning. Again, it's not perfect, but you can do something in that direction.
11:19:We can also do sentiment analysis, meaning, to figure out whether a sentence is positive or negative. This is especially useful for review analysis, for example.
11:30:So these are examples of semantic analysis. And they help us to obtain partial understanding of the sentences.
11:38:It's not giving us a complete understanding, as I showed it before, for this sentence. But it would still help us gain understanding of the content. And these can be useful.
11:51:In terms of inference, we are not there yet, probably because of the general difficulty of inference and uncertainties. This is a general challenge in artificial intelligence. Now that's probably also because we don't have complete semantical representation for natural [INAUDIBLE] text. So this is hard. Yet in some domains perhaps, in limited domains when you have a lot of restrictions on the word uses, you may be able to perform inference to some extent. But in general we can not really do that reliably. Speech act analysis is also far from being done and we can only do that analysis for very special cases. So this roughly gives you some idea about the state of the art. And then we also talk a little bit about what we can't do, and so we can't even do 100% part of speech tagging. Now this looks like a simple task, but think about the example here, the two uses of off may have different syntactic categories if you try to make a fine grained distinctions. It's not that easy to figure out such differences.
13:10:It's also hard to do general complete parsing. And again, the same sentence that you saw before is example.
13:18:This ambiguity can be very hard to disambiguate and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background, in order to figure out who actually had the telescope. So although the sentence looks very simple, it actually is pretty hard. And in cases when the sentence is very long, imagine it has four or five prepositional phrases, and there are even more possibilities to figure out.
13:48:It's also harder to do precise deep semantic analysis. So here's an example. In the sentence "John owns a restaurant." How do we define owns exactly? The word own, it is something that we can understand but it's very hard to precisely describe the meaning of own for computers.
14:11:So as a result we have a robust and a general Natural Language Processing techniques that can process a lot of text data.
14:22:In a shallow way, meaning we only do superficial analysis. For example, parts of speech tagging or a partial parsing or recognizing sentiment. And those are not deep understanding, because we're not really understanding the exact meaning of the sentence.
14:41:On the other hand of the deep understanding techniques tend not to scale up well, meaning that they would fill only some restricted text. And if you don't restrict the text domain or the use of words, then these techniques tend not to work well. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on. But they generally wouldn't work well on the data that are very different from the training data. So this pretty much summarizes the state of the art of Natural Language Processing. Of course, within such a short amount of time we can't really give you a complete view of NLP, which is a big field. And I'd expect to see multiple courses on Natural Language Processing topic itself. But because of its relevance to the topic that we talk about, it's useful for you to know the background in case you happen to be exposed to that. So what does that mean for Text Retrieval?
15:48:Well, in Text Retrieval we are dealing with all kinds of text. It's very hard to restrict text to a certain domain. And we also are often dealing with a lot of text data. So that means The NLP techniques must be general, robust, and efficient. And that just implies today we can only use fairly shallow NLP techniques for text retrieval. In fact, most search engines today use something called a bag of words representation.
16:20:Now, this is probably the simplest representation you can possibly think of. That is to turn text data into simply a bag of words. Meaning we'll keep individual words, but we'll ignore all the orders of words. And we'll keep duplicated occurrences of words. So this is called a bag of words representation. When you represent text in this way, you ignore a lot of valid information. That just makes it harder to understand the exact meaning of a sentence because we've lost the order.
16:53:But yet this representation tends to actually work pretty well for most search tasks. And this was partly because the search task is not all that difficult. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions.
17:13:So in comparison of some other tasks, for example, machine translation would require you to understand the language accurately. Otherwise the translation would be wrong. So in comparison such tasks are all relatively easy. Such a representation is often sufficient and that's also the representation that the major search engines today, like a Google or Bing are using.
17:35:Of course, I put in parentheses but not all, of course there are many queries that are not answered well by the current search engines, and they do require the replantation that would go beyond bag of words replantation. That would require more natural language processing to be done.
17:52:There was another reason why we have not used the sophisticated NLP techniques in modern search engines. And that's because some retrieval techniques actually, naturally solved the problem of NLP. So one example is word sense disintegration. Think about a word like Java. It could mean coffee or it could mean program language.
18:15:If you look at the word anome, it would be ambiguous, but when the user uses the word in the query, usually there are other words. For example, I'm looking for usage of Java applet. When I have applet there, that implies Java means program language. And that contest can help us naturally prefer documents which Java is referring to program languages. Because those documents would probably match applet as well. If Java occurs in that documents where it means coffee then you would never match applet or with very small probability. So this is the case when some retrieval techniques naturally achieve the goal of word.
19:01:Another example is some technique called feedback which we will talk about later in some of the lectures. This technique would allow us to add additional words to the query and those additional words could be related to the query words. And these words can help matching documents where the original query words have not occurred. So this achieves, to some extent, semantic matching of terms. So those techniques also helped us bypass some of the difficulties in natural language processing.
19:40:However, in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines. And it's particularly needed for complex search tasks.
19:52:Or for question and answering.
19:55:Google has recently launched a knowledge graph, and this is one step toward that goal, because knowledge graph would contain entities and their relations. And this goes beyond the simple bag of words replantation. And such technique should help us improve the search engine utility
20:14:significantly, although this is the open topic for research and exploration. In sum, in this lecture we talked about what is NLP and we've talked about the state of that techniques. What we can do, what we cannot do. And finally, we also explain why the bag of words replantation remains the dominant replantation used in modern search engines, even though deeper NLP would be needed for future search engines. If you want to know more, you can take a look at some additional readings. I only cited one here and that's a good starting point. Thanks. [MUSIC]
0:00:[SOUND] In this lecture,  we're going to talk about the text access.
0:14:In the previous lecture, we talked about the natural language content, analysis.
0:19:We explained that the state of the are natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner. As a result, bag of words remains very popular in applications like a search engine.
0:39:In this lecture, we're going to talk about some high-level strategies to help users get access to the text data. This is also important step to convert raw big text data into small random data. That are actually needed in a specific application. So the main question we'll address here, is how can a text information system, help users get access to the relevant text data? We're going to cover two complimentary strategies, push versus pull.
1:12:And then we're going to talk about two ways to implement the pull mode, querying versus browsing.
1:20:So first push versus pull.
1:24:These are two different ways connect the users with the right information at the right time.
1:31:The difference is which takes the initiative,
1:37:which party takes the initiative.
1:40:In the pull mode, the users take the initiative to start the information access process.
1:47:And in this case, a user typically would use a search engine to fulfill the goal. For example, the user may type in the query and then browse the results to find the relevant information.
2:02:So this is usually appropriate for satisfying a user's ad hoc information need.
2:10:An ad hoc information need is a temporary information need. For example, you want to buy a product so you suddenly have a need to read reviews about related product. But after you have cracked information, you have purchased in your product. You generally no longer need such information, so it's a temporary information need.
2:31:In such a case, it's very hard for a system to predict your need, and it's more proper for the users to take the initiative, and that's why search engines are very useful. Today because many people have many information needs all the time. So as we're speaking Google is probably processing many queries from this. And those are all, or mostly adequate. Information needs.
2:57:So this is a pull mode. In contrast in the push mode in the system would take the initiative to push the information to the user or to recommend information to the user. So in this case this is usually supported by a recommender system.
3:13:Now this would be appropriate if. The user has a stable information.
3:17:For example you may have a research interest in some topic and that interest tends to stay for a while. So, it's rather stable. Your hobby is another example of. A stable information need is such a case the system can interact with you and can learn your interest, and then to monitor the information stream. If the system hasn't seen any relevant items to your interest, the system could then take the initiative to recommend the information to you. So, for example, a news filter or news recommended system could monitor the news stream and identify interesting news to you and simply push the news articles to you.
3:59:This mode of information access may be also a property that when this system has good knowledge about the users need and this happens in the search context. So for example, when you search for information on the web a search engine might infer you might be also interested in something related. Formation. And they would recommend the information to you, so that just reminds you, for example, of an advertisement placed on the search page.
4:27:So this is about the two high level strategies or two modes of text access.
4:35:Now let's look at the pull mode in more detail.
4:39:In the pull mode, we can further distinguish it two ways to help users. Querying versus browsing. In querying, a user would just enter a query. Typical the keyword query, and the search engine system would return relevant documents to use.
4:54:And this works well when the user knows what exactly are the keywords to be used. So if you know exactly what you are looking for, you tend to know the right keywords. And then query works very well, and we do that all of the time.
5:09:But we also know that sometimes it doesn't work so well. When you don't know the right keywords to use in the query, or you want to browse information in some topic area. You use because browsing would be more useful. So in this case, in the case of browsing, the users would simply navigate it, into the relevant information by following the paths
5:34:supported by the structures of documents. So the system would maintain some kind of structures and then the user could follow these structures to navigate.
5:47:So this really works well when the user wants to explore the information space or the user doesn't know what are the keywords to using the query. Or simply because the user finds it inconvenient to type in a query. So even if a user knows what query to type in if the user is using a cellphone to search for information. It's still harder to enter the query. In such a case, again, browsing tends to be more convenient. The relationship between browsing and querying is best understood by making and imagine you're site seeing.
6:25:Imagine if you're touring a city. Now if you know the exact address of attraction.
6:31:Taking a taxi there is perhaps the fastest way. You can go directly to the site. But if you don't know the exact address, you may need to walk around. Or you can take a taxi to a nearby place and then walk around.
6:44:It turns out that we do exactly the same in the information studies. If you know exactly what you are looking for, then you can use the right keywords in your query to find the information you're after. That's usually the fastest way to do, find information.
6:59:But what if you don't know the exact keywords to use? Well, you clearly probably won't so well. You will not related pages. And then, you need to also walk around in the information space, meaning by following the links or by browsing. You can then finally get into the relevant page.
7:17:If you want to learn about again. You will likely do a lot of browsing so just like you are looking around in some area and you want to see some interesting attractions related in the same. [INAUDIBLE]. So this analogy also tells us that today we have very good support for query, but we don't really have good support for browsing. And this is because in order to browse effectively, we need a map to guide us, just like you need a map to. Of Chicago, through the city of Chicago, you need a topical map to tour the information space. So how to construct such a topical map is in fact a very interesting research question that might bring us more interesting browsing experience on the web or in applications.
8:19:So, to summarize this lecture, we've talked about the two high level strategies for text access; push and pull. Push tends to be supported by the Recommender System, and Pull tends to be supported by the Search Engine. Of course, in the sophisticated [INAUDIBLE] information system, we should combine the two.
8:38:In the pull mode, we can further this [INAUDIBLE] Querying and Browsing. Again we generally want to combine the two ways to help you assist, so that you can support the both querying nad browsing.
8:51:If you want to know more about the relationship between pull and push, you can read this article. This give excellent discussion of the relationship between machine filtering and information retrieval. Here informational filtering is similar to information recommendation or the push mode of information access. [MUSIC]
0:00:[MUSIC] This lecture is about the text retrieval problem.
0:12:This picture shows our overall plan for lectures.
0:16:In the last lecture, we talked about the high level strategies for text access. We talked about push versus pull.
0:25:Such engines are the main tools for supporting the pull mode. Starting from this lecture, we're going to talk about the how search engines work in detail.
0:38:So first it's about the text retrieval problem.
0:42:We're going to talk about the three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparison between Text Retrieval and the related task Database Retrieval.
0:58:Finally, we're going to talk about the Document Selection versus Document Ranking as two strategies for responding to a user's query.
1:09:So what is Text Retrieval?
1:12:It should be a task that's familiar for the most of us because we're using web search engines all the time.
1:19:So text retrieval is basically a task where the system would respond to a user's query With relevant documents. Basically, it's for supporting a query
1:32:as one way to implement the poll mode of information access.
1:39:So the situation is the following. You have a collection of text retrieval documents. These documents could be all the webpages on the web, or all the literature articles in the digital library. Or maybe all the text files in your computer.
1:58:A user will typically give a query to the system to express information need. And then, the system would return relevant documents to users. Relevant documents refer to those documents that are useful to the user who typed in the query.
2:16:All this task is a phone call that information retrieval.
2:21:But literally information retrieval would broadly include the retrieval of other non-textual information as well, for example audio, video, etc. It's worth noting that Text Retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data. So for example, current the image search engines actually match a user's query was the companion text data of the image.
2:59:This problem is also called search problem.
3:05:And the technology is often called the search technology industry.
3:11:If you ever take a course in databases it will be useful to pause the lecture at this point and think about the differences between text retrieval and database retrieval. Now these two tasks are similar in many ways.
3:29:But, there are some important differences.
3:33:So, spend a moment to think about the differences between the two. Think about the data, and the information managed by a search engine versus those that are managed by a database system.
3:47:Think about the different between the queries that you typically specify for database system versus queries that are typed in by users in a search engine.
3:59:And then finally think about the answers.
4:02:What's the difference between the two? Okay, so if we think about the information or data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages, etc.
4:31:The unstructured text is not obvious what are the names of people mentioned in the text.
4:40:Because of this difference, we also see that text information tends to be more ambiguous and we talk about that in the processing chapter, whereas in databases. But they don't tend to have where to find the semantics.
4:58:The results important difference in the queries, and this is partly due to the difference in the information or data.
5:07:So test queries tend to be ambiguous. Whereas in their research, the queries are typically well-defined. Think about a SQL query that would clearly specify what records to be returned. So it has very well-defined semantics.
5:27:Keyword queries or electronic queries tend to be incomplete, also in that it doesn't really specify what documents should be retrieved. Whereas complete specification for what should be returned.
5:47:And because of these differences, the answers would be also different. Being the case of text retrieval, we're looking for it rather than the documents.
5:58:In the database search, we are retrieving records or match records with the sequel query more precisely.
6:09:Now in the case of text retrieval, what should be the right answers to the query is not very well specified, as we just discussed.
6:21:So it's unclear what should be the right answers to a query. And this has very important consequences, and that is, textual retrieval is an empirically defined problem.
6:38:So this is a problem because if it's empirically defined, then we can not mathematically prove one method is better than another method.
6:52:That also means we must rely on empirical evaluation involving users to know which method works better.
7:02:And that's why we have. You need more than one lectures to cover the issue of evaluation. Because this is very important topic for Sir Jennings.
7:13:Without knowing how to evaluate heroism properly, there's no way to tell whether we have got the better or whether one system is better than another.
7:28:So now let's look at the problem in a formal way.
7:32:So, this slide shows a formal formulation of the text retrieval problem.
7:37:First, we have our vocabulary set, which is just a set of words in a language.
7:44:Now here, we are considering only one language, but in reality, on the web, there might be multiple natural languages. We have texts that are in all kinds of languages.
7:57:But here for simplicity, we just assume that is one kind of language. As the techniques used for retrieving data from multiple languages Are more or less similar to the techniques used for retrieving documents in one end, which although there is important difference, the principle methods are very similar.
8:21:Next, we have the query, which is a sequence of words.
8:26:And so here, you can see
8:31:the query is defined as a sequence of words. Each q sub i is a word in the vocabulary.
8:42:A document is defined in the same way, so it's also a sequence of words. And here, d sub ij is also a word in the vocabulary.
8:52:Now typically, the documents are much longer than queries.
8:57:But there are also cases where the documents may be very short.
9:04:So you can think about what might be a example of that case.
9:09:I hope you can think of Twitter search. Tweets are very short.
9:16:But in general, documents are longer than the queries.
9:22:Now, then we have a collection of documents, and this collection can be very large. So think about the web. It could be very large.
9:36:And then the goal of text retrieval is you'll find the set of relevant in the documents, which we denote by R'(q), because it depends on the query. And this in general, a subset of all the documents in the collection.
9:52:Unfortunately, this set of relevant documents is generally unknown, and user-dependent in the sense that, for the same query typed in by different users, they expect the relevant documents may be different.
10:09:The query given to us by the user is only a hint on which document should be in this set.
10:17:And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search, where the connection's so large, the user doesn't have complete knowledge about the whole production.
10:34:So the best search system can do is to compute an approximation of this relevant document set. So we denote it by R'(q). So formerly, we can see the task is to compute this R'(q) approximation of the relevant documents. So how can we do that? Now imagine if you are now asked to write a program to do this.
11:08:What would you do? Now think for a moment. Right, so these are your input. The query, the documents.
11:20:And then you are to compute the answers to this query, which is a set of documents that would be useful to the user.
11:29:So, how would you solve the problem? Now in general, there are two strategies that we can use.
11:39:The first strategy is we do a document selection, and that is, we're going to have a binary classification function, or binary classifier.
11:49:That's a function that would take a document and query as input, and then give a zero or one as output to indicate whether this document is relevant to the query or not.
12:02:So in this case, you can see the document.
12:08:The relevant document is set, is defined as follows. It basically, all the documents that have a value of 1 by this function.
12:25:So in this case, you can see the system must have decide if the document is relevant or not. Basically, it has to say whether it's one or zero. And this is called absolute relevance. Basically, it needs to know exactly whether it's going to be useful to the user.
12:41:Alternatively, there's another strategy called document ranking.
12:46:Now in this case, the system is not going to make a call whether a document is random or not. But rather the system is going to use a real value function, f here.
12:58:That would simply give us a value that would indicate which document is more likely relevant.
13:05:So it's not going to make a call whether this document is relevant or not. But rather it would say which document is more likely relevant. So this function then can be used to random documents, and then we're going to let the user decide where to stop, when the user looks at the document. So we have a threshold theta here to determine what documents should be in this approximation set. And we're going to assume that all the documents that are ranked above the threshold are in this set, because in effect, these are the documents that we deliver to the user. And theta is a cutoff determined by the user.
13:56:So here we've got some collaboration from the user in some sense, because we don't really make a cutoff. And the user kind of helped the system make a cutoff.
14:08:So in this case, the system only needs to decide if one document is more likely relevant than another. And that is, it only needs to determine relative relevance,
14:19:as opposed to absolute relevance.
14:22:Now you can probably already sense that relative relevance would be easier to determine than absolute relevance. Because in the first case, we have to say exactly whether a document is relevant or not.
14:37:And it turns out that ranking is indeed generally preferred to document selection.
14:46:So let's look at these two strategies in more detail. So this picture shows how it works. So on the left side, we see these documents, and we use the pluses to indicate the relevant documents. So we can see the true relevant documents here consists this set of true relevant documents, consists of these process, these documents.
15:17:And with the document selection function, we're going to basically classify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will not be perfect so it will make mistakes. So here we can see, in the approximation of the relevant documents, we have got some number in the documents.
15:43:And similarly, there is a relevant document that's misclassified as non-relevant. In the case of document ranking, we can see the system seems like, simply ranks all the documents in the descending order of the scores. And then, we're going to let the user stop wherever the user wants to stop. If the user wants to examine more documents, then the user will scroll down some more and then stop [INAUDIBLE]. But if the user only wants to read a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have delivered these four documents to our user.
16:33:So as I said ranking is generally preferred, and one of the reasons is because the classifier in the case of document selection is unlikely accurate. Why? Because the only clue is usually the query. But the query may not be accurate in the sense that it could be overly constrained.
16:57:For example, you might expect relevant documents to talk about all these
17:04:topics by using specific vocabulary. And as a result, you might match no relevant documents. Because in the collection, no others have discussed the topic using these vocabularies, right? So in this case, we'll see there is this problem of
17:25:no relevant documents to return in the case of over-constrained query.
17:33:On the other hand, if the query is under-constrained, for example, if the query does not have sufficient descriptive words to find the random documents. You may actually end up having of over delivery, and this when you thought these words my be sufficient to help you find the right documents. But, it turns out they are not sufficient and there are many distractions, documents using similar words. And so, this is a case of over delivery.
18:08:Unfortunately, it's very hard to find the right position between these two extremes.
18:15:Why? Because whether users looking for the information in general the user does not have a good knowledge about the information to be found. And in that case, the user does not have a good knowledge about what
18:30:vocabularies will be used in those relevent documents. So it's very hard for a user to pre-specify the right level of constraints.
18:44:Even if the classifier is accurate, we also still want to rend these relevant documents, because they are generally not equally relevant.
18:56:Relevance is often a matter of degree.
18:59:So we must prioritize these documents for a user to examine.
19:06:And note that this prioritization is very important
19:12:because a user cannot digest all the content the user generally would have to look at each document sequentially.
19:21:And therefore, it would make sense to users with the most relevant documents. And that's what ranking is doing. So for these reasons, ranking is generally preferred.
19:36:Now this preference also has a theoretical justification and this is given by the probability ranking principle.
19:44:In the end of this lecture, there is reference for this.
19:49:This principle says, returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions.
20:02:First, the utility of a document (to a user) Is independent of the utility of any other document.
20:10:Second, a user would be assumed to browse the results sequentially. Now it's easy to understand why these assumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utility of each document that's separate.
20:36:And this would allow the computer score for each document independently. And then, we are going to rank these documents based on the scrolls.
20:45:The second assumption is to say that the user would indeed follow the rank list. If the user is not going to follow the ranked list, is not going to examine the documents sequentially, then obviously the ordering would not be optimal.
21:00:So under these two assumptions, we can theoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold?
21:18:I suggest you to pause the lecture, for a moment, to think about this.
21:27:Now, can you think of some examples that would suggest these assumptions aren't necessarily true.
21:44:Now, if you think for a moment, you may realize none of the assumptions Is actually true.
21:53:For example, in the case of independence assumption we might have documents that have similar or exactly the same content. If we look at each of them alone, each is relevant.
22:07:But if the user has already seen one of them, we can assume it's generally not very useful for the user to see another similar or duplicated one.
22:19:So clearly the utility on the document that is dependent on other documents that the user has seen.
22:27:In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together. They provide answers to the user's question.
22:42:So this is a collective relevance and that also suggests that the value of the document might depend on other documents.
22:53:Sequential browsing generally would make sense if you have a ranked list there.
22:59:But even if you have a rank list, there is evidence showing that users don't always just go strictly sequentially through the entire list. They sometimes will look at the bottom for example, or skip some. And if you think about the more complicated interfaces that we could possibly use like two dimensional in the phase. Where you can put that additional information on the screen then sequential browsing is a very restricted assumption.
23:32:So the point here is that
23:35:none of these assumptions is really true but less than that.
23:41:But probability ranking principle establishes some solid foundation for
23:46:ranking as a primary pattern for search engines. And this has actually been the basis for a lot of research work in information retrieval. And many hours have been designed based on this assumption,
24:01:despite that the assumptions aren't necessarily true. And we can address this problem by doing post processing Of a ranked list, for example, to remove redundancy.
24:20:So to summarize this lecture, the main points that you can take away are the following. First, text retrieval is an empirically defined Problem. And that means which algorithm is better must be judged by the users. Second, document ranking is generally preferred. And this will help users prioritize examination of search results.
24:47:And this is also to bypass the difficulty in determining absolute relevance Because we can get some help from users in determining where to make the cut off, it's more flexible.
25:01:So, this further suggests that the main technical challenge in designing a search engine is the design effective ranking function.
25:10:In other words, we need to define what is the value of this function F on the query and document pair.
25:21:How we design such a function is the main topic in the following lectures.
25:29:There are two suggested additional readings. The first is the classical paper on the probability ranking principle.
25:37:The second one is a must-read for anyone doing research on information retrieval. It's a classic IR book, which has excellent coverage of the main research and results in early days up to the time when the book was written. Chapter six of this book has an in-depth discussion of the Probability Ranking Principle and Probably for retrieval models in general. [MUSIC]
0:00:[SOUND] This lecture is a overview of text retrieval methods.
0:13:In the previous lecture, we introduced the problem of text retrieval. We explained that the main problem is the design of ranking function to rank documents for a query. In this lecture, we will give an overview of different ways of designing this ranking function.
0:33:So the problem is the following. We have a query that has a sequence of words and the document that's also a sequence of words. And we hope to define a function f
0:45:that can compute a score based on the query and document. So the main challenge you hear is with design a good ranking function that can rank all the relevant documents on top of all the non-relevant ones. Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model, which gives us a formalization of relevance.
1:32:Now, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories.
1:42:First, one family of the models are based on the similarity idea.
1:50:Basically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vector space model, which we will cover more in detail later in the lecture.
2:20:A second kind of models are called probabilistic models. In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables.
2:36:And we assume there is a binary random variable called R here
2:42:to indicate whether a document is relevant to a query.
2:46:We then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query. There are different cases of such a general idea. One is classic probabilistic model, another is language model, yet another is divergence from randomness model.
3:12:In a later lecture, we will talk more about one case, which is language model. A third kind of model are based on probabilistic inference. So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document.
3:37:Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define a set of constraints that we hope a good retrieval function to satisfy.
3:55:So in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints.
4:05:Interestingly, although these different models are based on different thinking, in the end, the retrieval function tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the art retrieval model and to examine some of the common ideas used in all these models.
4:33:First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation used in all the search engines.
4:53:So with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based on scores computed based on each individual word.
5:09:And that means the score would depend on the score of each word, such as presidential, campaign, and news. Here, we can see there are three different components, each corresponding to how well the document matches each of the query words.
5:31:Inside of these functions, we see a number of heuristics used.
5:38:So for example, one factor that affects the function d here is how many times does the word presidential occur in the document? This is called a term frequency, or TF.
5:51:We might also denote as c of presidential and d. In general, if the word occurs more frequently in the document, then the value of this function would be larger. Another factor is, how long is the document? And this is to use the document length for scoring. In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document. Because in a long document, any term is expected to occur more frequently.
6:38:Finally, there is this factor called document frequency. That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential. And in some other models, we might also use a probability to characterize this information.
7:05:So here, I show the probability of presidential in the collection.
7:10:So all these are trying to characterize the popularity of the term in the collection. In general, matching a rare term in the collection is contributing more to the overall score than matching up common term.
7:25:So this captures some of the main ideas used in pretty much older state of the art original models.
7:34:So now, a natural question is, which model works the best?
7:39:Now it turns out that many models work equally well. So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these, BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers.
8:22:And we'll talk more about this method later in some other lectures.
8:30:So, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate retrieval model.
8:47:Second, many models are equally effective, but we don't have a single winner yet. Researchers are still active and working on this problem, trying to find a truly optimal retrieval model.
9:00:Finally, the state of the art ranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF and document frequency of words. Such information is used in the weighting function to determine the overall contribution of matching a word and document length. These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later.
9:36:There are two suggested additional readings if you have time.
9:41:The first is a paper where you can find the detailed discussion and comparison of multiple state of the art models.
9:49:The second is a book with a chapter that gives a broad review of different retrieval models. [MUSIC]
0:00:[SOUND]  This lecture is about the vector space retrieval model. We're going to give an introduction to its basic idea.
0:18:In the last lecture, we talked about the different ways of designing a retrieval model, which would give us a different arranging function.
0:30:In this lecture, we're going to talk about a specific way of designing a ramping function called a vector space retrieval model.
0:37:And we're going to give a brief introduction to the basic idea.
0:44:Vector space model is a special case of similarity based models as we discussed before. Which means we assume relevance is roughly similarity, between the document and the query.
1:02:Now whether is this assumption is true is actually a question. But in order to solve the search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the program analogy. So in this process, we have to make a number of assumptions. This is the first assumption that we make here. Basically, we assume that if a document is more similar to a query than another document. Then the first document will be assumed it will be more relevant than the second one. And this is the basis for ranking documents in this approach.
1:46:Again, it's questionable whether this is really the best definition for randoms. As we will see later there are other ways to model randoms.
1:58:The basic idea of vectors for base retrieval model is actually very easy to understand. Imagine a high dimensional space where each dimension corresponds to a term.
2:11:So here I issue a three dimensional space with three words, programming, library and presidential. So each term here defines one dimension.
2:24:Now we can consider vectors in this, three dimensional space. And we're going to assume that all our documents and the query will be placed in this vector space. So for example, on document might be represented by this vector, d1. Now this means this document probably covers library and presidential, but it doesn't really talk about programming. What does this mean in terms of representation of document? That just means we're going to look at our document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only the vector root condition of the document.
3:14:Of course, the document has all information. For example, the orders of words are [INAUDIBLE] model and that's because we assume that the [INAUDIBLE] of words will [INAUDIBLE]. So with this presentation you can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from another document which might be recommended as a different vector, d2 here. Now in this case, the document that covers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topic is likely about program language and the library is software lab library.
3:58:So this shows that by using this vector space reproduction, we can actually capture the differences between topics of documents.
4:09:Now you can also imagine there are other vectors. For example, d3 is pointing into that direction, that might be a presidential program. And in fact we can place all the documents in this vector space. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space, as another vector.
4:32:And then we're going to measure the similarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to be the closest to this query vector. And therefore, d2 will be rendered above others.
4:51:So this is basically the main idea of the vector space model.
4:58:So to be more precise, vector space model is a framework. In this framework, we make the following assumptions. First, we represent a document and query by a term vector.
5:18:So here a term can be any basic concept. For example, a word or a phrase or even n gram of characters. Those are just sequence of characters inside a word.
5:34:Each term is assumed that will be defined by one dimension. Therefore n terms in our vocabulary, we define N-dimensional space.
5:44:A query vector would consist of a number of elements
5:49:corresponding to the weights on different terms.
5:56:Each document vector is also similar. It has a number of elements and each value of each element is indicating the weight of the corresponding term. Here, you can see, we assume there are N dimensions. Therefore, they are N elements
6:15:each corresponding to the weight on the particular term.
6:21:So the relevance in this case will be assumed to be the similarity between the two vectors.
6:29:Therefore, our ranking function is also defined as the similarity between the query vector and document vector.
6:37:Now if I ask you to write a program to implement this approach in a search engine.
6:44:You would realize that this was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this. That's why I said, this is a framework.
6:59:And this has to be refined in order to actually
7:04:suggest a particular ranking function that you can implement on a computer.
7:10:So what does this framework not say? Well, it actually hasn't said many things that would be required in order to implement this function.
7:24:First, it did not say how we should define or select the basic concepts exactly.
7:32:We clearly assume the concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehow distinguish it as two different concepts. Then they would be defining two different dimensions and that would clearly cause redundancy here. Or all the emphasizing of matching this concept, because it would be as if you match the two dimensions when you actually matched one semantic concept.
8:11:Secondly, it did not say how we exactly should place documents and the query in this space. Basically that show you some examples of query and document vectors. But where exactly should the vector for a particular document point to?
8:29:So this is equivalent to how to define the term weights? How do you compute the lose element values in those vectors? This is a very important question, because term weight in the query vector indicates the importance of term.
8:48:So depending on how you assign the weight, you might prefer some terms to be matched over others.
8:56:Similarly, the total word in the document is also very meaningful. It indicates how well the term characterizes the document. If you got it wrong then you clearly don't represent this document accurately.
9:10:Finally, how to define the similarity measure is also not given. So these questions must be addressed before we can have a operational function that we can actually implement using a program language.
9:25:So how do we solve these problems is the main topic of the next lecture. [MUSIC]
0:00:In this lecture we're going to talk about how to instantiate vector space model so that we can get very specific ranking function.
0:22:So this is to continue the discussion of the vector space model, which is one particular approach to design a ranking function.
0:34:And we're going to talk about how we use the general framework of the the vector space model as a guidance to instantiate the framework to derive a specific ranking function. And we're going to cover the symbolist instantiation of the framework.
0:55:So as we discussed in the previous lecture, the vector space model is really a framework. And this didn't say.
1:05:As we discussed in the previous lecture, vector space model is really a framework. It does not say many things.
1:14:So, for example, here it shows that it did not say how we should define the dimension.
1:20:It also did not say how we place a document vector in this space.
1:27:It did not say how we place a query vector in this vector space.
1:32:And, finally, it did not say how we should measure the similarity between the query vector and the document vector.
1:40:So you can imagine, in order to implement this model,
1:46:we have to say specifically how we compute these vectors. What is exactly xi? And what is exactly yi?
1:58:This will determine where we place a document vector, where we place a query vector. And, of course, we also need to say exactly what should be the similarity function.
2:11:So if we can provide a definition of the concepts that would define the dimensions and these xi's or yi's and namely weights of terms for queries and document, then we will be able to place document vectors and query vectors in this well defined space. And then, if we also specify similarity function, then we'll have a well defined ranking function.
2:41:So let's see how we can do that and think about the instantiation. Actually, I would suggest you to pause the lecture at this point, spend a couple minutes to think about. Suppose you are asked to implement this idea.
2:59:You have come up with the idea of vector space model, but you still haven't figured out how to compute these vectors exactly, how to define the similarity function. What would you do?
3:12:So, think for a couple of minutes, and then proceed.
3:20:So, let's think about some simplest ways of instantiating this vector space model. First, how do we define the dimension? Well, the obvious choice is to use each word in our vocabulary to define the dimension. And show that there are N words in our vocabulary. Therefore, there are N dimensions. Each word defines one dimension. And this is basically the bag of words with
3:48:Now let's look at how we place vectors in this space.
3:54:Again here, the simplest strategy is to
3:58:use a Bit Vector to represent both the query and a document.
4:04:And that means each element, xi and yi will be taking a value of either zero or 1.
4:13:When it's 1, it means the corresponding word is present in the document or in the query. When it's 0, it's going to mean that it's absent.
4:27:So you can imagine if the user types in a few words in the query, then the query vector will only have a few 1's, many, many zeros.
4:37:The document vector, generally we have more 1's, of course. But it will also have many zeros since the vocabulary is generally very large. Many words don't really occur in any document.
4:52:Many words will only occasionally occur in a document.
4:58:A lot of words will be absent in a particular document.
5:04:So now we have placed the documents and the query in the vector space.
5:11:Let's look at how we measure the similarity.
5:15:So, a commonly used similarity measure here is Dot Product.
5:20:The Dot Product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. So, here we see that it's the product of x1 and y1. So, here. And then, x2 multiplied by y2. And then, finally, xn multiplied by yn. And then, we take a sum here.
5:50:So that's a Dot Product. Now, we can represent this in a more general way using a sum here.
5:58:So this is only one of the many different ways of measuring the similarity. So, now we see that we have defined the dimensions, we have defined the vectors, and we have also defined the similarity function. So now we finally have the simplest vector space model, which is based on the bit vector [INAUDIBLE] dot product similarity and bag of words [INAUDIBLE]. And the formula looks like this. So this is our formula. And that's actually a particular retrieval function, a ranking function right? Now we can finally implement this function using a program language, and then rank the documents for query. Now, at this point you should again pause the lecture to think about how we can interpreted this score. So, we have gone through the process of modeling the retrieval problem using a vector space model. And then, we make assumptions about how we place vectors in the vector space, and how do we define the similarity. So in the end, we've got a specific retrieval function shown here.
7:15:Now, the next step is to think about whether this retrieval function actually makes sense, right? Can we expect this function to actually perform well when we used it to rank documents for user's queries?
7:28:So it's worth thinking about what is this value that we are calculating. So, in the end, we'll get a number. But what does this number mean? Is it meaningful?
7:42:So, spend a couple minutes to sort of think about that.
7:45:And, of course, the general question here is do you believe this is a good ranking function? Would it actually work well? So, again, think about how to interpret this value. Is it actually meaningful?
8:01:Does it mean something? This is related to how well the document matched the query.
8:08:So, in order to assess whether this simplest vector space model actually works well, let's look at the example.
8:17:So, here I show some sample documents and a sample query. The query is news about the presidential campaign. And we have five documents here. They cover different terms in the query.
8:34:And if you look at these documents for a moment, you may realize that
8:41:some documents are probably relevant, and some others are probably not relevant.
8:48:Now, if I asked you to rank these documents, how would you rank them? This is basically our ideal ranking. When humans can examine the documents, and then try to rank them.
9:03:Now, so think for a moment, and take a look at this slide. And perhaps by pausing the lecture.
9:12:So I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well. They match news, presidential and campaign.
9:27:So, it looks like these documents are probably better than the others. They should be ranked on top. And the other three d2, d1, and d5 are really not relevant. So we can also say d4 and d3 are relevant documents, and d1, d2 and d5 are non-relevant. So now let's see if our simplest vector space model could do the same, or could do something closer. So, let's first think about how we actually use this model to score documents. All right. Here I show two documents, d1 and d3. And we have the query also here. In the vector space model, of course we want to first compute the vectors for these documents and the query. Now, I showed the vocabulary here as well. So these are the end dimensions that we'll be thinking about. So what do you think is the vector for the query?
10:27:Note that we're assuming that we only use zero and 1 to indicate whether a term is absent or present in the query or in the document. So these are zero,1 bit vectors.
10:43:So what do you think is the query vector?
10:47:Well, the query has four words here. So for these four words, there will be a 1. And for the rest, there will be zeros.
10:57:Now, what about the documents? It's the same. So d1 has two rows, news and about. So, there are two 1's here, and the rest are zeroes. Similarly, so now that we have the two vectors, let's compute the similarity.
11:17:And we're going to use Do Product. So you can see when we use Dot Product, we just multiply the corresponding elements, right? So these two will be formal product, and these two will generate another product, and these two will generate yet another product and so on, so forth.
11:40:Now you can easily see if we do that, we actually don't have to care about
11:48:these zeroes because whenever we have a zero the product will be zero. So when we take a sum over all these pairs, then the zero entries will be gone.
12:04:As long as you have one zero, then the product would be zero. So, in the fact, we're just counting how many pairs of 1 and 1. In this case, we have seen two, so the result will be 2. So what does that mean? Well, that means this number, or the value of this scoring function, is simply the count of how many unique query terms are matched in the document. Because if a term is matched in the document, then there will be two one's.
12:41:If it's not, then there will be zero on the document side.
12:46:Similarly, if the document has a term but the term is not in the query, there will be a zero in the query vector. So those don't count. So, as a result, this scoring function basically measures how many unique query terms are matched in a document. This is how we interpret this score.
13:07:Now, we can also take a look at d3. In this case, you can see the result is 3 because d3 matched to the three distinctive query words news, presidential campaign, whereas d1 only matched the two. Now in this case, this seems reasonable to rank d3 on top of d1.
13:29:And this simplest vector space model indeed does that. So that looks pretty good. However, if we examine this model in detail, we likely will find some problems. So, here I'm going to show all the scores for these five documents. And you can easily verify they're correct because we're basically counting the number of unique query terms matched in each document.
13:56:Now note that this measure actually makes sense, right? It basically means if a document matches more unique query terms, then the document will be assumed to be more relevant. And that seems to make sense. The only problem is here we can note that there are three documents, d2, d3 and d4. And they tied with a 3 as a score.
14:25:So, that's a problem because if you look at them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once, but d4 mentioned it multiple times. In the case of d3, presidential could be an dimension. But d4 is clearly above the presidential campaign. Another problem is that d2 and d3 also have the same score. But if you look at the three words that are matched, in the case of d2, it matched the news, about and campaign. But in the case of d3, it matched news, presidential and campaign.
15:12:So intuitively this reads better because matching presidential is more important than matching about, even though about and the presidential are both in the query.
15:26:So intuitively, we would like d3 to be ranked above d2. But this model doesn't do that.
15:33:So that means this model is still not good enough. We have to solve these problems.
15:41:To summarize, in this lecture we talked about how to instantiate a vector space model.
15:47:We mainly need to do three things. One is to define the dimension. The second is to decide how to place documents as vectors in the vector space, and to also place a query in the vector space as a vector.
16:07:And third is to define the similarity between two vectors, particularly the query vector and the document vector.
16:17:We also talked about various simple way to instantiate the vector space model. Indeed, that's probably the simplest vector space model that we can derive. In this case, we use each word to define the dimension. We use a zero, 1 bit vector to represent a document or a query. In this case, we basically only care about word presence or absence. We ignore the frequency.
16:45:And we use the Dot Product as the similarity function.
16:50:And with such a instantiation, we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document.
17:04:We also showed that such a simple vector space model still doesn't work well, and we need to improve it.
17:12:And this is a topic that we're going to cover in the next lecture. [MUSIC]
0:00:[SOUND] In this lecture, we are going to talk about how to improve the instantiation of the vector space model.
0:17:This is a continued discussion of the vector space model. We're going to focus on how to improve the instantiation of this model.
0:30:In the previous lecture, you have seen that with simple instantiations of the vector space model, we can come up with a simple scoring function that would give us basically an account of how many unique query terms are matched in the document.
0:50:We also have seen that this function has a problem, as shown on this slide. In particular, if you look at these three documents, they will all get the same score because they match the three unique query words.
1:06:But intuitively we would like d4 to be ranked above d3, and d2 is really not relevant.
1:14:So the problem here is that this function couldn't capture the following heuristics. First, we would like to give more credit to d4 because it matched presidential more times than d3.
1:32:Second, intuitively, matching presidential should be more important than matching about, because about is a very common word that occurs everywhere. It doesn't really carry that much content.
1:47:So in this lecture, let's see how we can improve the model to solve these two problems. It's worth thinking at this point about why do we have these problems?
2:01:If we look back at assumptions we have made while instantiating the vector space model, we'll realize that the problem is really coming from some of the assumptions. In particular, it has to do with how we placed the vectors in the vector space.
2:22:So then naturally, in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different ways to instantiate the vector space model. In particular, we have to place the vectors in a different way.
2:41:So let's see how we can improve this. One natural thought is in order to consider multiple times of a term in the document, we should consider the term frequency instead of just the absence or presence. In order to consider the difference between a document where a query term occurred multiple times and one where the query term occurred just once, we have to consider the term frequency, the count of a term in the document.
3:13:In the simplest model, we only modeled the presence and absence of a term. We ignored the actual number of times that a term occurs in a document. So let's add this back. So we're going to then represent a document by a vector with term frequency as element. So that is to say, now the elements of both the query vector and the document vector will not be 0 or 1s, but instead they will be the counts of a word in the query or the document.
3:52:So this would bring in additional information about the document, so this can be seen as more accurate representation of our documents. So now let's see what the formula would look like if we change this representation. So as you'll see on this slide, we still use dot product.
4:10:And so the formula looks very similar in the form. In fact, it looks identical. But inside the sum, of course, x i and y i are now different. They are now the counts of word i in the query and in the document. Now at this point I also suggest you to pause the lecture for a moment and just to think about how we can interpret the score of this new function. It's doing something very similar to what the simplest VSM is doing. But because of the change of the vector, now the new score has a different interpretation. Can you see the difference? And it has to do with the consideration of multiple occurrences of the same term in a document. More importantly, we would like to know whether this would fix the problems of the simplest vector space model. So let's look at this example again. So suppose we change the vector representation to term frequency vectors. Now let's look at these three documents again. The query vector is the same because all these words occurred exactly once in the query. So the vector is still a 01 vector. And in fact, d2 is also essentially representing the same way because none of these words has been repeated many times. As a result, the score is also the same, still 3.
5:45:The same is true for d3, and we still have a 3.
5:51:But d4 would be different, because now presidential occurred twice here. So the ending for presidential in the document vector would be 2 instead of 1.
6:04:As a result, now the score for d4 is higher. It's a 4 now.
6:10:So this means by using term frequency, we can now rank d4 above d2 and d3, as we hoped to.
6:19:So this solved the problem with d4.
6:26:But we can also see that d2 and d3 are still filtering the same way. They still have identical scores, so it did not fix the problem here.
6:40:So how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about. But how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly and which word can be basically ignored? About is such a word which does not really carry that much content. We can essentially ignore that. We sometimes call such a word a stock word. Those are generally very frequent and they occur everywhere. Matching it doesn't really mean anything. But computationally how can we capture that?
7:24:So again, I encourage you to think a little bit about this.
7:29:Can you came up with any statistical approaches to somehow distinguish presidential from about?
7:37:Now if you think about it for a moment, you'll realize that one difference is that a word like above occurs everywhere. So if you count the occurrence of the word in the whole collection, then we will see that about has much higher frequency than presidential, which tends to occur only in some documents.
8:01:So this idea suggests that we could somehow use the global statistics of terms or some other information to trying to down-weight the element of about in a vector representation of d2. At the same time, we hope to somehow increase the weight of presidential in the vector of d3. If we can do that, then we can expect that d2 will get the overall score to be less than 3 while d3 will get the score above 3. Then we would be able to rank d3 on top of d2.
8:45:So how can we do this systematically?
8:48:Again, we can rely on some statistical count. And in this case, the particular idea is called inverse document frequency. Now we have seen document frequency as one signal used in the modern retrieval functions.
9:05:We discussed this in a previous lecture. So here is the specific way of using it. Document frequency is the count of documents that contain a particular term. Here we say inverse document frequency because we actually want to reward a word that doesn't occur in many documents.
9:24:And so the way to incorporate this into our vector representation is then to modify the frequency count by multiplying it by the IDF of the corresponding word, as shown here. If we can do that, then we can penalize common words, which generally have a lower IDF, and reward rare words, which will have a higher IDF. So more specifically, the IDF can be defined as the logarithm of M+1 divided by k, where M is the total number of documents in the collection, k is the DF or document frequency, the total number of documents containing the word W. Now if you plot this function by varying k, then you would see the curve would look like this. In general, you can see it would give a higher value for a low DF word, a rare word.
10:34:You can also see the maximum value of this function is log of M+1.
10:40:It would be interesting for you to think about what's the minimum value for this function. This could be an interesting exercise.
10:50:Now the specific function may not be as important as the heuristic to simply penalize popular terms.
11:01:But it turns out that this particular function form has also worked very well.
11:07:Now whether there's a better form of function here is the open research question. But it's also clear that if we use a linear penalization, like what's shown here with this line, then it may not be as reasonable as the standard IDF.
11:29:In particular, you can see the difference in the standard IDF,
11:35:and we somehow have a turning point of here.
11:41:After this point, we're going to say these terms are essentially not very useful. They can be essentially ignored. And this makes sense when the term occurs so frequently and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important and it's basically a common term.
12:03:It's not very important to match this word. So with the standard IDF you can see it's basically assumed that they all have low weights. There's no difference. But if you look at the linear penalization, at this point that there is still some difference. So intuitively we'd want to focus more on the discrimination of low DF words rather than these common words.
12:32:Well, of course, which one works better still has to be validated by using the empirically correlated dataset. And we have to use users to judge which results are better.
12:48:So now let's see how this can solve problem 2. So now let's look at the two documents again.
12:56:Now without the IDF weighting before, we just have term frequency vectors. But with IDF weighting we now can adjust the TF weight by multiplying with the IDF value. For example, here we can see is adjustment and in particular for about there's adjustment by using the IDF value of about, which is smaller than the IDF value of presidential. So if you look at these, the IDF will distinguish these two words. As a result, adjustment here would be larger, would make this weight larger.
13:37:So if we score with these new vectors, then what would happen is that, of course, they share the same weights for news and campaign, but the matching of about will discriminate them. So now as a result of IDF weighting, we will have d3 to be ranked above d2 because it matched a rare word, whereas d2 matched a common word. So this shows that the IDF weighting can solve problem 2.
14:12:So how effective is this model in general when we used TF-IDF weighting? Well, let's look at all these documents that we have seen before. These are the new scores of the new documents. But how effective is this new weighting method and new scoring function point?
14:33:So now let's see overall how effective is this new ranking function with TF-IDF weighting.
14:40:Here we show all the five documents that we have seen before, and these are their scores.
14:47:Now we can see the scores for the first four documents here seem to be quite reasonable. They are as we expected.
14:58:However, we also see a new problem because now d5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score. In fact, it has the highest score here.
15:16:So this creates a new problem. This is actually a common phenomenon in designing retrieval functions. Basically, when you try to fix one problem, you tend to introduce other problems. And that's why it's very tricky how to design effective ranking function. And what's the best ranking function is their open research question. Researchers are still working on that.
15:42:But in the next few lectures we're going to also talk about some additional ideas to further improve this model and try to fix this problem.
15:55:So to summarize this lecture, we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TD-IDF weighting. So the improvement is mostly on the placement of the vector where we give high weight to a term that occurred many times in a document but infrequently in the whole collection.
16:23:And we have seen that this improved model indeed looks better than the simplest vector space model. But it also still has some problems. In the next lecture we're going to look at how to address these additional problems. [MUSIC]
0:00:[MUSIC]
0:10:In this lecture, we continue the discussion of vector space model. In particular, we're going to talk about the TF transformation. In the previous lecture, we have derived a TF idea of weighting formula using the vector space model.
0:27:And we have assumed that this model actually works pretty well for these examples as shown on this slide, except for d5, which has received a very high score. Indeed, it has received the highest score among all these documents. But this document is intuitive and non-relevant, so this is not desirable.
0:53:In this lecture, we're going to talk about, how we're going to use TF transformation to solve this problem.
1:00:Before we discuss the details, let's take a look at the formula for this simple TF-IDF weighting ranking function. And see why this document has received such a high score. So this is the formula, and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.
1:23:And inside the sum, each matched query term has a particular weight. And this weight is TF-IDF weighting.
1:31:So it has an idea of component, where we see two variables. One is the total number of documents in the collection, and that is M. The other is the document of frequency. This is the number of documents that are contained. This word w. The other variables involved in the formula include the count of the query term.
2:01:W in the query, and the count of the word in the document.
2:07:If you look at this document again, now it's not hard to realize that the reason why it hasn't received a high score is because it has a very high count of campaign. So the count of campaign in this document is a 4, which is much higher than the other documents, and has contributed to the high score of this document. So in treating the amount to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. And if you think about the matching of terms in the document carefully, you actually would realize, we probably shouldn't reward multiple occurrences so generously. And by that I mean, the first occurrence of a term says a lot about the matching of this term, because it goes from zero count to a count of one. And that increase means a lot.
3:17:Once we see a word in the document, it's very likely that the document is talking about this word. If we see a extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say that, well the second occurrence kind of confirmed that it's not a accidental managing of the word. Now we are more sure that this document is talking about this word. But imagine we have seen, let's say, 50 times of the word in the document. Now, adding one extra occurrence is not going to test more about the evidence, because we're already sure that this document is about this word.
4:01:So if you're thinking this way, it seems that we should restrict the contribution of a high count of a term, and that is the idea of TF Transformation. So this transformation function is going to turn the real count of word into a term frequency weight for the word in the document. So here I show in x axis that we'll count, and y axis I show the term frequency weight.
4:33:So in the previous breaking functions, we actually have imprison rate use some kind of transformation. So for example, in the 0/1 bit vector recantation,
4:44:we actually use such a transformation function, as shown here. Basically if the count is 0, then it has 0 weight, otherwise it would have a weight of 1. It's flat.
4:59:Now, what about using term count as TF weight? Well, that's a linear function, so it has just exactly the same weight as the count.
5:11:Now we have just seen that this is not desirable.
5:18:So what we want is something like this. So for example, with an algorithm function, we can't have a sublinear transformation that looks like this. And this will control the influence of really high weight, because it's going to lower its inference. Yet, it will retain the inference of small counts.
5:36:Or we might want to even bend the curve more by applying logarithm twice.
5:42:Now people have tried all these methods. And they are indeed working better than the linear form of the transformation.
5:50:But so far, what works the best seems to be this special transformation, called a BM25 transformation.
5:58:BM stands for best matching.
6:01:Now in this transformation, you can see there's a parameter k here.
6:06:And this k controls the upper bound of this function. It's easy to see this function has a upper bound, because if you look at the x divided by x + k, where k is a non-active number, then the numerator will never be able to exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference between this transformation function and a logarithm transformation.
6:37:Which it doesn't have upper bound.
6:39:Furthermore, one interesting property of this function is that, as we vary k,
6:45:we can actually simulate different transformation functions. Including the two extremes that are shown here. That is, the 0/1 bit transformation and the linear transformation. So for example, if we set k to 0, now you can see
7:03:the function value will be 1. So we precisely recover the 0/1 bit transformation.
7:15:If you set k to very large number on the other hand, it's going to look more like the linear transformation function.
7:24:So in this sense, this transformation is very flexible. It allows us to control the shape of the transformation. It also has a nice property of the upper bound.
7:38:And this upper bound is useful to control the inference of a particular term.
7:43:And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term.
7:57:In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to compute the score.
8:06:As I said, this transformation function has worked well so far.
8:12:So to summarize this lecture, the main point is that we need to do Sublinear TF Transformation, and this is needed to capture the intuition of diminishing return from higher term counts.
8:26:It's also to avoid the dominance by one single term over all others. This BM25 transformation that we talked about is very interesting. It's so far one of the best-performing TF Transformation formulas. It has upper bound, and so it's also robust and effective.
8:47:Now if we're plugging this function into our TF-IDF weighting vector space model. Then we'd end up having the following ranking function, which has a BM25 TF component.
9:01:Now, this is already very close to a state of the odd ranking function called BM25. And we'll discuss how we can further improve this formula in the next lecture. [MUSIC]
0:00:[SOUND]
0:08:This lecture is about Document Length Normalization in the Vector Space Model. In this lecture, we will continue the discussion of the vector space model. In particular, we're going to discuss the issue of document length normalization.
0:25:So far in the lectures about the vector space model, we have used the various signals from the document to assess the matching of the document with a query. In particular, we have considered the tone frequency. The count of a tone in a document. We have also considered it's global statistics such as, IDF, Inverse Document Frequency. But we have not considered document lengths.
0:54:So here I show two example documents, d4 is much shorter with only 100 words.
1:01:D6 on the other hand, has a 5000 words. If you look at the matching of these query words, we see that in d6, there are more matchings of the query words. But one might reason that, d6 may have matched these query words in a scattered manner.
1:24:So maybe the topic of d6, is not really about the topic of the query.
1:31:So, the discussion of the campaign at the beginning of the document, may have nothing to do with the managing of presidential at the end.
1:40:In general, if you think about the long documents, they would have a higher chance for matching any query. In fact, if you generate a long document randomly by assembling words from a distribution of words, then eventually you probably will match an inquiry.
2:00:So in this sense, we should penalize on documents because they just naturally have better chance matching to any query, and this is idea of document normalization.
2:12:We also need to be careful in avoiding to over penalize long documents.
2:19:On the one hand, we want to penalize the long document. But on the other hand, we also don't want to over-penalize them. Now, the reasoning is because a document that may be long because of different reasons.
2:32:In one case, the document may be long because it uses more words.
2:38:So for example, think about the vortex article on the research paper. It would use more words than the corresponding abstract.
2:49:So, this is a case where we probably should penalize the matching of
2:54:long documents such as a full paper. When we compare the matching of words in such a long document with matching of the words in the shop abstract.
3:07:Then long papers in general, have a higher chance of matching clearer words, therefore, we should penalize them. However, there is another case when the document is long, and that is when the document simply has more content. Now consider another case of long document, where we simply concatenate a lot of abstracts of different papers. In such a case, obviously, we don't want to over-penalize such a long document. Indeed, we probably don't want to penalize such a document because it's long.
3:39:So that's why, we need to be careful about using the right degree of penalization.
3:48:A method of that has been working well, based on recent results, is called a pivoted length normalization. And in this case, the idea is to use the average document length as a pivot, as a reference point. That means we'll assume that for the average length documents, the score is about right so the normalizer would be 1. But if the document is longer than the average document length,
4:14:then there will be some penalization. Whereas if it's a shorter, then there is even some reward. So this is illustrated at using this slide, on the axis, x-axis you can see the length of document. On the y-axis, we show the normalizer. In this case, the Pivoted Length Normalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in length controlled by a parameter B here.
4:53:So you can see here, when we first divide the length of the document by the average documents, this not only gives us some sense about how this document is compared with average documents, but also gives us a benefit of not worrying about the unit of length. We can measure the length by words or by characters.
5:20:Anyway, this normalizer has interesting property. First we see that, if we set the parameter b to 0 then the value would be 1. So, there's no lens normalization at all. So, b, in this sense, controls the lens normalization.
5:39:Whereas, if we set b to a nonzero value, then the normalizer would look like this. All right, so the value would be higher for documents that are longer than the average document lens.
5:53:Whereas, the value of the normalizer would be shorter, would be smaller for shorter documents. So in this sense, we see there is a penalization for long documents, and there's a reward for short documents.
6:09:The degree of penalization is controlled by b, because if we set b to a larger value, then the normalizer would look like this. There's even more penalization for long documents and more reward for the short documents. By adjusting b, which varies from 0 to 1, we can control the degree of length normalization. So, if we plug in this length normalization fact that into the vector space model, ranking functions is that we have already examined them.
6:41:Then we will end up having the following formulas.
6:46:And these are in fact the state of the vector space model formulas. Let's take a look at each of them. The first one is called a pivoted length normalization vector space model, and a reference in [INAUDIBLE] duration of this model. And here we see that, it's basically a TFI model that we have discussed, the idea of component should be very familiar to you.
7:18:There is also a query term frequency component here.
7:24:And then, in the middle, there is the normalizer tf and in this case, we see we use the double logarithm as we discussed before and this is to achieve a sublinear transformation. But we also put a document the length normalizer in the bottom. Right, so this would cause penalization for long document, because the larger the denominator is, then the smaller the is. And this is of course controlled by the parameter b here.
8:01:And you can see again, if b is set to 0 then there is no length normalization.
8:08:Okay, so this is one of the two most effective at these base model formulas. The next one called a BM25 or Okapi, is also similar in that it also has a IDF component here, and query IDF component here.
8:32:But in the middle, the normal issue's a little bit different. As we explained, there is our copy tf transformation here, and that does sublinear transformation with the upper bound.
8:48:In this case we have put the length normalization factor here. We're adjusting k but it achieves a similar factor, because we put a normalizer in the denominator. Therefore, again, if a document is longer then the term weight will be smaller.
9:10:So you can see after we have gone through all the n answers that we talked about, and we have in the end reached the basically the state of god functions. So, So far, we have talked about mainly how to place the document vector in the vector space.
9:35:And, this has played an important role in determining the effectiveness of the simple function. But there are also other dimensions, where we did not really examine details. For example, can we further improve the instantiation of the dimension of the Vector Space Model? Now, we've just assumed that the bag of words representation should issue dimension as a word but obviously, we can see there are many other choices. For example, a stemmed word, those are the words that haven't transformed into the same root form, so that computation and computing were all become the same and they can be match. We get those stop word removal. This is to remove some very common words that don't carry any content like the off.
10:26:We get use of phrases to define dimensions. We can even use later in the semantical analysis, it will find some clusters of words that represent the a late in the concept as one by an engine.
10:39:We can also use smaller unit, like a character end grams those are sequences of and the characters for dimensions.
10:50:However, in practice, people have found that the bag-of-words representation with phrases is still the most effective one and it's also efficient. So, this is still so far the most popular dimension instantiation method.
11:10:And it's used in all major search engines.
11:13:I should also mention, that sometimes we need to do language specific and domain specific tokenization. And this is actually very important, as we might have variations of terms that might prevent us from matching them with each other, even when they mean the same thing. In some languages like Chinese, there is also the challenge in segmenting
11:40:text to obtain word band rates because it's just a sequence of characters. A word might correspond to one character or two characters or even three characters. So, it's easier in English when we have a space to separate the words. In some other languages, we may need to do some Americanize processing to figure a way out of what are the boundaries for words. There is also the possibility to improve the similarity of the function. And so far we have used as a top product, but one can imagine there are other measures. For example, we can measure the cosine of the angle between two vectors. Or we can use Euclidean distance measure.
12:24:And these are all possible, but dot product seems still the best and one reason is because it's very general.
12:33:In fact that it's sufficiently general, if you consider the possibilities of doing waiting in different ways.
12:44:So, for example, cosine measure can be thought of as the thought product of two normalized factors. That means, we first normalize each factor and then we take the thought product. That would be critical to the cosine measure. I just mentioned that the BM25, seems to be one of the most effective formulas.
13:04:But there has been also further developments in improving BM25. Although, none of these words have changed the BM25 fundamental. So in one line work, people have divide the BM25 F. Here, F stands for field, and this is use BM25 for documents with structures. So for example, you might consider a title field, the abstract, or body of the research article. Or even anchor text on the web page, those are the text fields that describe links to other pages and these can all be combined with a proper way of different fields to help improve scoring for different documents. When we use BM25 for such a document and the obvious choice is to apply BM25 for each field and then combine the scores. Basically, the idea of BM25F is to first combine the frequency counts of terms in all the fields, and then apply BM25. Now, this has advantage of avoiding over counting the first occurrence of the term. Remember in the sublinear transformation of TF, the first occurrence is very important and it contributes a large weight. And if we do that for all the fields, then the same term might have gained a lot of advantage in every field. But when we combine these word frequencies together, we just do the transformation one time. At that time, then the extra occurrences will not be counted as fresh first recurrences.
14:48:And this method has been working very well for scoring structure with documents.
14:55:The other line of extension is called a BM25+. In this line, risk is to have to address the problem of over penalization of long documents by BM25.
15:08:So to address this problem, the fix is actually quite simple. We can simply add a small constant to the TF normalization formula. But what's interesting is that, we can analytically prove that by doing such a small modification, we will fix the problem of over penalization of law documents by the original BM25. So the new formula called BM25+, is empirically and analytically shown to be better than BM25.
15:42:So to summarize all what we have said about vector space model, here are the major take away points. First, in such a model, we use the similarity of relevance. Assuming that relevance of a document with respect to a query, is
16:02:basically proportional to the similarity between the query and the document. So naturally, that implies that the query and document must have been represented in the same way. And in this case, we will present them as vectors in high-dimensional vector space. Where the dimensions are defined by words, or concepts, or terms, in general.
16:25:And we generally, need to use a lot of heuristics to design the ranking function. We use some examples, which show the needs for several heuristics, including Tf weighting and transformation.
16:38:And IDF weighting, and document length normalization. These major heuristics are the most important of heuristics, to ensure such a general ranking function to work well for all kinds of test. And finally, BM25 and pivoted normalization seem to be the most effective formulas out of the vector space model. Now I have to say that, I put BM25 in the category of vector space model, but in fact, the BM25 has been derived using probabilistic model.
17:11:So the reason why I've put it in the vector space model is first, the ranking function actually has a nice interpretation in the vector space model. We can easily see, it looks very much like a vector space model, with a special waiting function.
17:28:The second reason is because the original BM25, has somewhat different form of IDF.
17:36:And that form of IDF after the [INAUDIBLE] doesn't work so well as the standard IDF that you have seen here. So as effective retrieval function, BM25 should probably use a heuristic modification of the IDF. To make them even more look like a vector space model
17:59:There are some additional readings. The first is, a paper about the pivoted length normalization. It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derive the length normalization formula. The second, is the original paper where the BM25 was proposed.
18:24:The third paper, has a thorough discussion of BM25 and its extensions, particularly BM25 F.
18:32:And finally, in the last paper has a discussion of improving BM25 to correct the over penalization of long documents. [MUSIC]
0:00:[MUSIC] This lecture is about the implementation of text retrieval systems.
0:12:In this lecture we will discuss how we can implement a text retrieval method to build a search engine. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. This is a typical text retrieval system architecture. We can see the documents are first processed by a tokenizer to get tokenized units, for example, words. And then, these words, or tokens, will be processed by a indexer that will create a index, which is a data structure for the search engine to use to quickly answer a query. And the query would be going through a similar processing step. So the Tokenizer would be apprised of the query as well, so that the text can be processed in the same way. The same units would be matched with each other. The query's representation would then be given to the Scorer, which would use the index to quickly answer user's query by scoring the documents and then ranking them. The results will be given to the user. And then the user can look at the results and provided us some feedback that can be explicit judgements of both which documents are good, which documents are bad. Or implicit feedback such as so that user didn't have to do anything extra. End user will just look at the results, and skip some, and click on some result to view. So these interacting signals can be used by the system to improve the ranking accuracy by assuming that viewed documents are better than the skipped ones. So a search engine system then can be divided into three parts. The first part is the indexer, and the second part is a Scorer that responds to the users query, and the third part is a Feedback mechanism. Now typically, the Indexer is done in the offline manner, so you can pre-process the correct data and to build the inventory index, which we will introduce in moment. And this data structure can then be used by the online module which is a scorer to process a user's query dynamically and quickly generate search results. The feedback mechanism can be done online or offline, depending on the method. The implementation of the indexer and the scorer is very standard, and this is the main topic of this lecture and the next few lectures. The feedback mechanism, on the other hand, has variations, it depends on which method is used. So that is usually done in algorithms specific way. Let's first talk about the tokenizer. Tokernization is a normalized lexical units in through the same form, so that semantically similar words can be matched with each other. Now, in the language like English, stemming is often used and this will map all the inflectional forms of words into the same root form. So for example, computer, computation, and computing can all be matched to the root form compute. This way all these different forms of computing can be matched with each other. Now normally, this is a good idea, to increase the coverage of documents that are matched up with this query. But it's also not always beneficial, because sometimes the subtlest difference between computer and computation might still suggest the difference in the coverage of the content. But in most cases, stemming seems to be beneficial. When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries. Because it's not obvious where the boundary is as there's no space to separate them. So here of course, we have to use some language specific processing techniques. Once we do tokenization, then we would index the text documents and than it'll convert the documents and do some data structure that can enable faster search. The basic idea is to precompute as much as we can basically. So the most commonly used index is call an Inverted index. And this has been used in many search engines to support basic search algorithms. Sometimes the other indices, for example, document index might be needed in order to support feedback, like I said. And these kind of techniques are not really standard in that they vary a lot according to the feedback methods. To understand why we want to use inverted index it will be useful for you to think about how you would respond to a single term query quickly. So if you want to use more time to think about that, pause the video. So think about how you can pre process the text data so that you can quickly respond to a query with just one word. Where if you have thought about that question, you might realize that where the best is to simply create the list of documents that match every term in the vocabulary. In this way, you can basically pre-construct the answers. So when you see a term you can simply just to fetch the random list of documents for that term and return the list to the user. So that's the fastest way to respond to a single term here. Now the idea of the invert index is actually, basically, like that. We're going to do pre-constructed search an index, that will allows us to quickly find all the documents that match a particular term. So let's take a look at this example. We have three documents here, and these are the documents that you have seen in some previous lectures. Suppose that we want to create an inverted index for these documents. Then we want to maintain a dictionary, in the dictionary we will have one entry for each term and we're going to store some basic statistics about the term. For example, the number of documents that match the term, or the total number of code or frequency of the term, which means we would kind of duplicate the occurrences of the term. And so, for example, news, this term occur in all the three documents, so the count of documents is three. And you might also realize we needed this count of documents, or document frequency, for computing some statistics to be used in the vector space model. Can you think of that? So what weighting heuristic would need this count. Well, that's the idea, right, inverse document frequency. So, IDF is the property of a term, and we can compute it right here. So, with the document that count here, it's easy to compute the idea of, either at this time, or with the old index, or. At random time when we see a query. Now in addition to these basic statistics, we'll also store all the documents that matched the news, and these entries are stored in the file called Postings.
8:24:So in this case it matched three documents and we store information about these three documents here. This is the document id, document 1 and the frequency is 1. The tf is one for news, in the second document it's also 1, et cetera. So from this list, we can get all the documents that match the term news and we can also know the frequency of news in these documents. So, if the query has just one word, news, and we have easily look up to this table to find the entry and go quicker into the postings to fetch all the documents that matching yours. So, let's take a look at another term.
9:09:This time, let's take a look at the word presidential.
9:14:This would occur in only one document, document 3. So the document frequency is 1 but it occurred twice in this document. So the frequency count is two, and the frequency count is used for some other reachable method where we might use the frequency to
9:34:assess the popularity of a term in the collection. Similarly we'll have a pointer to the postings here, and in this case, there is only one entry here because
9:48:the term occurred in just one document and that's here. The document id is 3 and it occurred twice.
9:59:So this is the basic idea of inverted index. It's actually pretty simple, right?
10:06:With this structure we can easily fetch all the documents that match a term. And this will be the basis for scoring documents for a query. Now sometimes we also want to store the positions of these terms.
10:25:So in many of these cases the term occurred just once in the document. So there's only one position for example in this case.
10:35:But in this case, the term occurred twice so there's two positions. Now the position information is very useful for the checking whether the matching of query terms is actually within a small window of, let's say, five words or ten words.
10:52:Or, whether the matching of the two query terms is, in fact, a phrase of two words. That this can all be checked quickly by using the position from each.
11:05:So, why is inverted index good for fast search? Well, we just talked about the possibility of using the two answer single-term query. And that's very easy. What about the multiple term queries? Well let's first look at the some special cases of the Boolean query. A Boolean query is basically a Boolean expression like this. So I want the value in the document to match both term A and term B. So that's one conjunctive query. Or I want the web documents to match term A or term B. That's a disjunctive query. But how can we answer such a query by using inverted index?
11:52:Well if you think a bit about it, it would be obvious because we have simply fetch all the documents that match term A and also fetch all the documents that match term B. And then just take the intersection to answer a query like A and B. Or to take the union to answer the query A or B. So this is all very easy to answer. It's going to be very quick. Now what about the multi-term keyword query? We talked about the vector space model for example and we will do a match such query with document and generate the score. And the score is based on aggregated term weights. So in this case it's not the Boolean query but the scoring can be actually done in similar way. Basically it's similar to disjunctive Boolean query. Basically, it's like A or B. We take the union of all the documents that match at least one query term and then we would aggregate the term weights. So this is a basic idea of using inverted index for scoring documents in general. And we're going to talk about this in more detail later. But for now, let's just look at the question why is in both index, a good idea? Basically why is more efficient than sequentially just scanning documents. This is the obvious approach. You can just compute a score for each document and then you can then sort them. And this is a straightforward method but this is going to be very slow imagine the wealth, there's a lot of documents. If you do this then it will take a long time to answer your query. So the question now is why would the invert index be much faster? Well it has to do is the word distribution in text. So, here's some common phenomena of word distribution in the text. There are some languages independent of patterns that seem to be stable.
14:00:And these patterns are basically characterized by the following pattern. A few words like the common words like the, a, or we occur very, very frequently in text. So they account for a large percent of occurrences of words.
14:19:But most words would occur just rarely. There are many words that occur just once, let's say, in a document or once in the collection. And there are many such. It's also true that the most frequent the words in one corpus they have to be rare in another. That means although the general phenomenon is applicable, was observed in many cases that exact words that are common may vary from context to context. So this phenomena is characterized by what's called a Zipf's Law. This law says that the rank of a word multiplied by the frequency of the word is roughly constant.
15:07:So formally if we use F(w) to denote the frequency, r(w) to denote the rank of a word. Then this is the formula. It basically says the same thing, just mathematical term. Where C is basically a constant and so, and there is also a parameter, alpha, that might be adjusted to better fit any empirical observations. So if I plot the word frequencies in sorted order, then you can see this more easily. The x axis is basically the word rank. This is r(w) and the y axis is word frequency or F(w). Now this curve shows that the product of the two is roughly the constant. Now if you look at these words, we can see They can be separated into three groups. In the middle, it's the intermediary frequency words. These words tend to occur quite in a few documents, but they are not like those most frequent words. And they are also not very rare.
16:18:So they tend to be often used in
16:22:queries and they also tend to have high TF-IDF weights. These intermediate frequency words. But if you look at the left part of the curve,
16:35:these are the highest frequency words. They are covered very frequently. They are usually words, like the, we, of Etc. Those words are very, very frequent and they are in fact the two frequent to be discriminated, and they are generally not very useful for retrieval. So they are often removed and this is called the stop words removal. So you can use pretty much just the kind of words in the collection to kind of infer what words might be stop words. Those are basically the highest frequency words.
17:13:And they also occupy a lot of space in the inverted index. You can imagine the posting entries for such a word would be very long. And then therefore, if you can remove such words you can save a lot of space in the inverted index.
17:29:We also show the tail part, which has a lot of rare words. Those words don't occur very frequently, and there are many such words.
17:39:Those words are actually very useful for search also, if a user happens to be interested in such a topic. But because they're rare, it's often true that users aren't necessarily interested in those words. But retain them would allow us to match such a document accurately. They generally have very high IDF.
18:05:So what kind of data structures should we use to store inverted index? Well, it has two parts, right. If you recall, we have a dictionary and we also have postings. The dictionary has modest size, although for the web it's still going to be very large but compare it with postings it's more distinct.
18:26:And we also need to have fast random access to the entries because we're going to look up on the query term very quickly. So therefore, we'd prefer to keep such a dictionary in memory if it's possible. If the collection is not very large, this is feasible, but if the collection is very large then it's in general not possible. If the vocabulary size is very large, obviously we can't do that. So, in general that's how it goes. So the data structures that we often use for storing dictionary, it would be direct access. There are structures like hash table, or b-tree if we can't store everything in memory or use disk. And then try to build a structure that would allow it to quickly look up entries.
19:14:For postings they are huge.
19:18:And in general, we don't have to have direct access to a specific entry. We generally would just look up a sequence of document IDs and frequencies for all the documents that matches the query term.
19:33:So would read those entries sequentially.
19:37:And therefore because it's large and we generally have store postings on disc, they have to stay on disc and they would contain information such as document IDs, term frequency or term positions, etcetera. Now because they are very large, compression is often desirable.
19:59:Now this is not only to save disc space, and this is of course one benefit of compression, it It's not going to occupy that much space. But it's also to help improving speed.
20:13:Can you see why? Well, we know that input and output would cost a lot of time. In comparison with the time taken by CPU. So, CPU is much faster but IO takes time and so by compressing the inverter index, opposing files will become smaller, and the entries, that we have the readings, and memory to process a query term, would be smaller, and then, so we can reduce the amount of tracking IO and that can save a lot of time. Of course, we have to then do more processing of the data when we uncompress the data in the memory. But as I said CPU is fast. So over all we can still save time.
21:08:So compression here is both to save disc space and to speed up the loading of the index. [MUSIC]
0:00:[SOUND]
0:07:This lecture is about the inverted index construction.
0:13:In this lecture, we will continue the discussion of system implementation. In particular, we're going to discuss how to construct the inverted index.
0:25:The construction of the inverted index is actually very easy if the dataset is very small. It's very easy to construct a dictionary and then store the postings in a file.
0:36:The problem is that when our data is not able to fit to the memory then we have to use some special method to deal with it.
0:46:And unfortunately, in most retrieval applications the dataset will be large. And they generally cannot be loaded into memory at once.
0:56:And there are many approaches to solve that problem, and sorting-based method is quite common and works in four steps as shown here. First, you collect the local termID, documentID and frequency tuples. Basically you will locate the terms in a small set of documents. And then once you collect those accounts you can sort those count based on terms. So that you will be able to local a partial inverted index and these are called rounds. And then you write them into a temporary file on the disk and then you merge in step 3. Do pairwise merging of these runs, until you eventually merge all the runs and generate a single inverted index.
1:47:So this is an illustration of this method. On the left you see some documents and on the right we have a term lexicon and a document ID lexicon. These lexicons are to map string-based representations of document IDs or terms into integer representations or map back from integers to the stream representation. The reason why we want our interest using integers to present these IDs is because integers are often easier to handle. For example, integers can be used as index for array, and they are also easy to compress.
2:34:So this is one reason why we tend to map these strings into integers,
2:42:so that we don't have to carry these strings around. So how does this approach work? Well, it's very simple. We're going to scan these documents sequentially and then parse the documents and count the frequencies of terms. And in this stage we generally sort the frequencies by document IDs, because we process each document sequentially. So we'll first encounter all the terms in the first document. Therefore the document IDs are all ones in this case. And this will be followed by document IDs two and they are natural results in this only just because we process the data in a sequential order. At some point, we will run out of memory and that would have to write them into the disc. Before we do that we 're going to sort them, just use whatever memory we have. We can sort them and then this time we're going to sort based on term IDs. Note that here, we're using the term IDs as a key to sort. So all the entries that share the same term would be grouped together. In this case, we can see all the IDs of documents that match term 1 would be grouped together. And we're going to write this into that this is a temporary file. And would that allows you to use the memory to process and makes a batch of documents. And we're going to do that for all the documents. So we're going to write a lot of temporary files into the disc. And then the next stage is we do merge sort basically. We're going to merge them and then sort them. Eventually, we will get a single inverted index, where the entries are sorted based on term IDs.
4:46:And on the top, we're going to see these are the older entries for the documents that match term ID 1. So this is basically, how we can do the construction of inverted index. Even though the data cannot be all loaded into the manner. Now, we mention earlier that because of hostings are very large, it's desirable to compress them. So let's now take a little bit how we compressed inverted index. Well the idea of compression in general, is for leverage skewed distributions of values. And we generally have to use variable-length encoding, instead of the fixed-length encoding as we use by default in a program manager like C++. And so how can we leverage the skewed distributions of values to compress these values? Well in general, we will use few bits to encode those frequent words at the cost of using longer bit string code those rare values. So in our case, let's think about how we can compress the TF, tone frequency.
6:05:Now, if you can picture what the inverted index look like, and you will see in post things, there are a lot of tone frequencies. Those are the frequencies of terms in all those documents. Now, if you think about it, what kind of values are most frequent there? You probably will be able to guess that small numbers tend to occur far more frequently than large numbers. Why? Well, think about the distribution of words and this is to do the sip of slopes, and many words occur just rarely so we see a lot of small numbers. Therefore, we can use fewer bits for the small, but highly frequent integers and that's cost of using more bits for larger integers.
6:58:This is a trade off of course. If the values are distributed to uniform, then this won't save us any space, but because we tend to see many small values, they are very frequent. We can save on average even though sometimes when we see a large number we have to use a lot of bits.
7:19:What about the document IDs that we also saw in postings? Well they are not distributed in the skewed way. So how can we deal with that? Well it turns out that we can use a trick called a d-gap and that is to store the difference of these term IDs. And we can imagine if a term has matched that many documents then there will be longest of document IDs. So when we take the gap, and we take the difference between adjacent document IDs, those gaps will be small. So again, see a lot of small numbers. Whereas if a term occurred in only a few documents, then the gap would be large, the large numbers would not be frequent. So this creates some skewed distribution, that would allow us to compress these values.
8:11:This is also possible because in order to uncover or uncompress these document IDs, we have to sequentially process the data. Because we stored the difference and in order to recover the exact document ID we have to first recover the previous document ID. And then we can add the difference to the previous document ID to restore the current document ID. Now this was possible because we only needed to have sequential access to those document IDs. Once we look up the term, we look up all the document IDs that match the term, then we sequentially process them. So it's very natural, that's why this trick actually works.
8:53:And there are many different methods for encoding. So binary code is a commonly used code in just any program language. We use basically fixed glance in coding. Unary code, gamma code, and delta code are all possibilities and there are many other possibilities. So let's look at some of them in more detail. Binary coding is really equal length coding, and that's a property for randomly distributed values. The unary coding is a variable length in coding method. In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0. So for example, 3 will be encoded as 2, 1s followed by 0, whereas 5 will be encoded as 4, 1s, followed by 0, etc. So now you can imagine how many bits do we have to use for a large number like 100? So how many bits do you have to use exactly for a number like 100? Well exactly, we have to use 100 bits. So it's the same number of bits as the value of this number. So this is very inefficient if you were likely to see some large numbers. Imagine if you occasionally see a number like 1,000, you have to use 1,000 bits. So this only works well if you are absolutely sure that there will be no large numbers, mostly very often you see very small numbers. Now, how do you decode this code? Now since these are variable length encoding methods, you can't just count how many bits and then just stop.
10:38:You can't say 8-bits or 32-bits, then you will start another code. They are variable length, so you will have to rely on some mechanism. In this case for unary, you can see it's very easy to see the boundary. Now you can easily see 0 would signal the end of encoding. So you just count up how many 1s you have seen and at the end you hit 0. You have finished one number, you will start another number.
11:07:Now we just saw that unary coding is too aggressive. In rewarding small numbers, and if you occasionally can see a very big number, it would be a disaster. So what about some other less aggressive method? Well gamma coding's one of them and in this method we can use unary coding for a transform form of that. So it's 1 plus the floor of log of x. So the magnitude of this value is much lower than the original x. So that's why we can afford using unary code for that. And so first I have the unary code for coding this log of x. And this would be followed by a uniform code or binary code. And this basically the same uniform code, and binary code are the same. And we're going to use this coder to code the remaining part of the value of x. And this is basically precisely x-1 to the floor of log of x
12:25:So the unary code are basically called the flow of log of x, well add one there and here. But the remaining part we'll be using uniform code through actually code the difference between the x and this 2 to the log of x.
12:49:And it's easy to show that for this
12:55:difference we only need to use up to this many bits and the floor of log of x bits.
13:06:And this is easy to understand, if the difference is too large, then we would have a higher floor of log of x.
13:14:So here are some examples for example, 3 is is encoded as 101. The first two digits are the unary code. So this isn't for the value 2, 10 encodes 2 in unary coding.
13:32:And so that means the floor of log of x is 1, because we won't actually use unary codes. In code 1 plus the flow of log of x, since this is two then we know that the flow of log of x is actually 1.
13:52:So that 3 is still larger than 2 to the 1. So the difference is 1, and the 1 is encoded here at the end.
14:01:So that's why we have 101 for 3. Now similarly 5 is encoded as 110, followed by 01.
14:12:And in this case the unary code in code 3. And so this is a unary code 110 and so the flow of log of x is 2. And that means we're going to compute a difference between 5 and the 2 to the 2 and that's 1. And so we now have again 1 at the end. But this time we're going to use 2 bits, because with this level of flow of log of x. We could have more numbers a 5, 6, 7 they would all share the same prefix here, 110. So in order to differentiate them, we have to use 2 bits in the end to differentiate them. So you can imagine 6 would be 10 here in the end instead of 01 after 10.
15:04:It's also true that the form of a gamma code is always the first odd number of bits, and in the center there is a 0. That's the end of the unary code.
15:18:And before that or on the left side of this 0, there will be all 1s. And on the right side of this 0, it's binary coding or uniform coding.
15:32:So how can you decode such code? Well you again first do unary coding. Once you hit 0, you have got the unary code and this also tell you how many bits you have to read further to decode the uniform code. So this is how you can decode a gamma code. There is also a delta code that's basically the same as a gamma code except that you replace the unary prefix with the gamma code. So that's even less conservative than gamma code in terms of wording the small integers. So that means, it's okay if you occasionally see a large number.
16:14:It's okay with delta code.
16:16:It's also fine with the gamma code, it's really a big loss for unary code. And they are all operating of course, at different degrees of favoring short or favoring small integers. And that also means they would be appropriate for a sorting distribution. But none of them is perfect for all distributions. And which method works the best would have to depend on the actual distribution in your dataset. For inverted index compression, people have found that gamma coding seems to work well.
16:55:So how to uncompress inverted index? I will just talk about this. Firstly, you decode those encoded integers. And we just I think discussed the how we decode unary coding and gamma coding. What about the document IDs that might be compressed using d-gap? Well, we're going to do sequential decoding so supposed the encoded I list is x1, x2, x3 etc. We first decode x1 to obtain the first document ID, ID1. Then we can decode x2, which is actually the difference between the second ID and the first one. So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this secondary position.
17:46:So this is where you can see the advantages of converting document IDs to integers. And that allows us to do this kind of compression. And we just repeat until we decode all the documents. Every time we use the document ID in the previous position to help to recover the document ID in the next position.
18:08:[MUSIC]
0:00:[SOUND] This lecture is about how to do faster search by using invert index.
0:14:In this lecture, we're going to continue the discussion of system implementation. In particular, we're going to talk about how to support a faster search by using invert index.
0:26:So let's think about what a general scoring function might look like.
0:32:Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form.
0:42:So the form of this function is as follows.
0:46:We see this scoring function of a document D and a query Q is defined as first a function of fa that adjustment a function that would consider two factors. That I'll assume here at the end, f sub d of d and f sub q of q. These are adjustment factors of a document and a query, so they are at the level of a document and the query. So and then inside of this function, we also see there's another function called h. So this is the main part of the scoring function and these as I just said of the scoring factors at the level of the whole document and the query. For example, document [INAUDIBLE] and this aggregate punching would then combine all these. Now inside this h function, there are functions that would compute the weights of the contribution of a matched query term ti.
2:08:So this g, the function g gives us the weight of a matched query term ti in document d.
2:23:And this h function would then aggregate all these weights. So for example, take a sum of all the matched query terms,
2:36:but it can also be a product or it could be another way of aggregating them.
2:41:And then finally, this adjustment the functioning would then consider the document level or query level factors to further adjust this score, for example, document [INAUDIBLE]. So, this general form would cover many state of [INAUDIBLE] functions. Let's look at how we can score documents with such a function using virtual index.
3:07:So, here's a general algorithm that works as follows. First this query level and document level factors can be pre-computed in the indexing time. Of course, for the query we have to compute it at the query time but for document, for example, document [INAUDIBLE] can be pre-computed. And then, we maintain a score accumulator for each document d to computer h.
3:34:An h is an aggregation function over all the matching query terms. So how do we do that? For each period term we're going to do fetch the inverted list from the invert index. This will give us all the documents that match this query term
3:52:and that includes d1, f1 and so dn fn. So each pair is a document ID and the frequency of the term in the document. Then for each entry d sub j and f sub j are particular match of the term in this particular document d sub j. We'll going to compute the function g that would give us something like weight of this term, so we're computing the weight completion of matching this query term in this document. And then, we're going to update the score accumulator for this document and this would allow us to add this to our accumulator that would incrementally compute function h. So this is basically a general way to allow pseudo computer or functions of this form by using the inbound index. Note that we don't have to attach any of document and that didn't match any query term. Well, this is why it's fast, we only need to process the documents that matched at least one query term. In the end, then we're going to adjust the score the computer this function f sub a and then we can sort. So let's take a look at a specific example. In this case, let's assume the scoring function is a very simple one, it just takes the sum of t f, the role of t f, the count of a term in the document.
5:25:This simplification would help shield the algorithm clearly. It's very easy to extend the computation to include other weights like the transformation of tf, or [INAUDIBLE] or IDF [INAUDIBLE]. So let's take a look at specific example, where the queries information security
5:48:and it show some entries of invert index on the right side. Information occurred in four documents and their frequencies are also there, security occurred in three documents. So let's see how the arrows works, so first we iterate overall query terms and we fetch the first query then, what is that? That's information, right? And imagine we have all these score accumulators who score the,
6:17:scores for these documents. We can imagine there will be other but then they will only be allocated as needed. So before we do any waiting of terms, we don't even need a score of. That comes actually we have these score accumulators eventually allocating.
6:38:So lets fetch the interest from the entity [INAUDIBLE] for information, that the first one.
6:46:So these four accumulators obviously would be initialize as zeros.
6:51:So, the first entry is d1 and 3, 3 is occurrences of information in this document. Since our scoring function assume that the score is just a sum of these raw counts. We just need to add a 3 to the score accumulator to account for the increase of score due to matching this term information, a document d1. And then, we go to the next entry, that's d2 and 4 and then we add a 4 to the score accumulator of d2. Of course, at this point, that we will allocate the score accumulator as needed. And so at this point, we allocated the d1 and d2, and the next one is d3, and we add one, we allocate another score [INAUDIBLE] d3 and add one to it. And then finally, the d4 gets a 5, because the term information occurred five times in this document. Okay, so this completes the processing of all the entries in the invert index for information. It processed all the contributions of matching information in this four documents.
8:01:So now, our error will go to the next that's security. So, we're going to fetch all the inverted index entries for security.
8:10:So, in this case, there are three entries, and we're going to go through each of them. The first is d2 and 3 and that means security occur three humps in d2 and what do we do? Well, we do exactly the same, as what we did for information. So, this time we're going to change the score [INAUDIBLE] d2 since it's already allocated and what we do is we'll add 3 to the existing value which is a 4, so we now get a 7 for d2.
8:41:D2 score is increased because the match that falls the information and the security. Go to the next entry, that's d4 and 1, so we would the score for d4 and again, we add 1 to d4 so d4 goes from 5 to 6. Finally, we process d5 and a 3. Since we have not yet allocated a score accumulated for d5, at this point, we're going to allocate 1 for d5, and we're going to add a 3 to it. So, those scores, of the last rule, are the final scores for these documents.
9:20:If our scoring function is just a simple some of TF values.
9:27:Now, what if we, actually, would like to do form addition? Well, we going to do the [INAUDIBLE] at this point, for each document.
9:36:So, to summarize this, all right, so you can see, we first process the information determine query term information and we processed all the entries in what index for this term. Then we process the security, all right, its worst think about what should be the order of processing here when we can see the query terms? It might make a difference especially if we don't want to keep all the score accumulators. Let's say, we only want to keep the most promising score accumulators. What do you think would be a good order to go through? Would you process a common term first or would you process a rare term first?
10:24:The answers is we just go to who should process the rare term first. A rare term would match a few documents, and then the score contribution would be higher, because the ideal value would be higher. And then, it allows us to attach the most diplomacy documents first. So, it helps pruning some non-promising ones, if we don't need so many documents to be returned to the user. So those are all heuristics for further improving the accuracy. Here you can also see how we can incorporate the idea of waiting, right? So they can [INAUDIBLE] when we incorporate [INAUDIBLE] when we process each query time. When we fetch the inverted index we can fetch the document frequency and then we can compute IDF. Or maybe perhaps the IDF value has already been precomputed when we indexed the documents. At that time, we already computed the IDF value that we can just fetch it, so all these can be done at this time. So that would mean when we process all the entries for information, these words would be adjusted by the same IDF, which is IDF for information.
11:36:So this is the basic idea of using inverted index for fast research and it works well for all kinds of formulas that are of the general form. And this generally, the general form covers actually most state of art retrieval functions. So there are some tricks to further improve the efficiency, some general techniques to encode the caching. This is we just store some results of popular queries, so that next time when you see the same query, you simply return the stored results. Similarly, you can also slow the list of inverted index in the memory for a popular term. And if the query term is popular likely, you will soon need to factor the inverted index for the same term again. So keeping it in the memory would help, and these are general techniques for improving efficiency. We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents. We only need to return high qualities subset of documents that likely are ranked on the top.
12:47:For that purpose, we can then prune the accumulators. We don't have to store all the accumulators. At some point, we just keep the highest value accumulators. Another technique is to do parallel processing and that's needed for really process in such a large data set like the web data set. And you scale up to the Web-scale really to have the special techniques you do parallel processing and to distribute the storage of files on machines. So here is a list of some text retrieval toolkits, it's not a complete list. You can find more information at this URL on the bottom. And here, I listed your four here, Lucene's one of the most popular toolkits that can support a lot of applications and it has very nice support for applications. You can use it to build a search engine application very quickly. The downside is that it's not that easy to extend it, and the algorithms implemented they are also not the most advanced algorithms. Lemur or Indri is another toolkit that does not have such a nice support web application as Lucene but it has many advanced search algorithms and it's also easy to extend. Terrier is yet another toolkit that also has good support for application capability and some advanced algorithms. So that's maybe in between Lemur or Lucene, or maybe rather combining the strands of both, so that's also useful tool kit. MeTA is a toolkit that we will use for the problem assignment and this is a new toolkit that has
14:47:a combination of both text retrieval algorithms and text mining algorithms. And so talking models are implement they are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms. So to summarize all the discussion about the System Implementation,
15:11:here are the major takeaway points. Inverted index is the primary data structure for supporting a search engine and that's the key to enable faster response to a user's query.
15:26:And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate. So that we can save disk space and we can speed up IO and processing of inverted index in general. We talked about how to construct the invert index when the data can't fit into the memory. And then we talk about faster search using that index basically, what's we exploit the invective index to accumulate a scores for documents [INAUDIBLE] algorithm. And we exploit the Zipf's law to avoid the touching many documents that don't match any query term and this algorithm can actually support a wide range of ranking algorithms.
16:13:So these basic techniques have great potential for further scaling up using distributed file system, parallel processing, and caching. Here are two additional readings you can take a look, if you have time and you are interested in learning more about this. The first one is a classical textbook on the efficiency o inverted index and the compression techniques. And how to, in general feel that the efficient any inputs of the space, overhead and speed. The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.
16:58:[MUSIC]
0:00:[MUSIC]
0:07:This lecture is about Evaluation of Text Retrieval Systems In the previous lectures, we have talked about the a number of Text Retrieval Methods, different kinds of ranking functions.
0:23:But how do we know which one works the best? In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods.
0:34:So this is the main topic of this lecture.
0:40:First, lets think about why do we have to do evaluation? I already give one reason. That is, we have to use evaluation to figure out which retrieval method works better. Now this is very important for advancing our knowledge. Otherwise, we wouldn't know whether a new idea works better than an old idea. In the beginning of this course, we talked about the problem of text retrieval. We compare it with data base retrieval.
1:08:There we mentioned that text retrieval is an empirically defined problem. So evaluation must rely on users. Which system works better, would have to be judged by our users.
1:25:So, this becomes a very challenging problem because how can we get users involved in the evaluation? How can we do a fair comparison of different method?
1:37:So just go back to the reasons for evaluation.
1:41:I listed two reasons here. The second reason, is basically what I just said, but there is also another reason which is to assess the actual utility of a Text Regional system. Imagine you're building your own such annual applications, it would be interesting knowing how well your search engine works for your users. So in this case, matches must reflect the utility to the actual users in real occasion. And typically, this has to be done by using user starters and using the real search engine.
2:16:In the second case, or the second reason,
2:19:the measures actually all need to collated with the utility to actually use this. Thus, they don't have to accurately reflect the exact utility to users.
2:31:So the measure only needs to be good enough to tell which method works better.
2:38:And this is usually done through a test collection. And this is the main idea that we'll be talking about in this course. This has been very important for comparing different algorithms and for improving search engine system in general.
2:58:So let's talk about what to measure. There are many aspects of searching that we can measure, we can evaluate. And here, I listed the three major aspects. One, is effectiveness or accuracy. How accurate are the search results? In this case, we're measuring a system's capability of ranking relevant documents on top of non relevant ones. The second, is efficiency. How quickly can you get the results? How much computing resources are needed to answer a query? In this case, we need to measure the space and time overhead of the system.
3:32:The third aspect is usability. Basically the question is, how useful is a system for new user tasks. Here, obviously, interfaces and many other things also important and would typically have to do user studies.
3:47:Now in this course, we're going to talk mostly about effectiveness and accuracy measures. Because the efficiency and usability dimensions are not really unique to search engines. And so they are needed for without any other software systems. And there is also good coverage of such and other causes. But how to evaluate search engine's quality or accuracy is something unique to text retrieval and we're going to talk a lot about this. The main idea that people have proposed before using a test set to evaluate the text retrieval algorithm is called the Cranfield Evaluation Methodology. This one actually was developed a long time ago, developed in 1960s. It's a methodology for laboratory test of system components.
4:45:Its sampling methodology that has been very useful, not just for search engine evaluation. But also for evaluating virtually all kinds of empirical tasks, and for example in natural language processing or in other fields where the problem is empirical to find, we typically would need to use such a methodology. And today with the big data challenging with the use of machine learning everywhere. This methodology has been very popular, but it was first developed for a search engine application in the 1960s. So the basic idea of this approach is to build a reusable test collection and define measures.
5:27:Once such a test collection is built, it can be used again and again to test different algorithms. And we're going to define measures that allow you to quantify performance of a system and algorithm.
5:41:So how exactly will this work? Well we can do have a sample collection of documents and this is adjusted to simulate the real document collection in the search application. We're going to also have a sample set of queries, or topics. This is a little simulator that uses queries.
5:56:Then, we'll have to have those relevance judgments. These are judgments of which documents should be returned for which queries. Ideally, they have to be made by users who formulated the queries. Because those are the people that know exactly what documents would be used for. And finally, we have to have matches for quantify how well our system's result matches the ideal ranked list. That would be constructed base on user's relevance judgements. So this methodology is very useful for starting retrieval algorithms, because the test can be reused many times. And it will also provide a fair comparison for all the methods. We have the same criteria or same dataset to be used to compare different algorithms. This allows us to compare a new algorithm with an old algorithm that was divided many years ago, by using the same standard. So this is the illustration of this works, so as I said, we need our queries that are showing here. We have Q1, Q2 etc. We also need the documents and that's called the document caching and on the right side you will see we need relevance judgments. These are basically the binary judgments of documents with respect to a query. So for example, D1 is judged as being relevant to Q1, D2 is judged as being relevant as well, and D3 is judged as not relevant. And the Q1 etc. These will be created by users.
7:34:Once we have these, and we basically have a test collection. And then if you have two systems, you want to compare them, then you can just run each system on these queries and the documents and each system would then return results. Let's say if the queries Q1 and then we would have the results here. Here I show R sub A as the results from system A. So this is, remember we talked about task of computing approximation of the relevant document set. R sub A is system A's approximation here.
8:14:And R sub B is system B's approximation of relevant documents.
8:21:Now, let's take a look at these results. So which is better? Now imagine if a user, which one would you like? Now let's take a look at the both results. And there are some differences and there are some documents that are returned by both systems. But if you look at the results, you will feel that maybe A is better in the sense that we don't have many number element documents. And among the three documents returned, the two of them are relevant. So that's good, it's precise. On the other hand one council say maybe B is better, because we've got all of them in the documents. We've got three instead of two. So which one is better and how do we quantify this?
9:08:Well, obviously this question highly depends on a user's task. It depends on users as well. You might even imagine for some users may be system A is better. If the user is not interested in getting all the random documents. Right, in this case the user doesn't have to read a million users will see most of the relevant documents. On the other hand, one can also imagine the user might need to have as many random documents as possible. For example, if you're doing a literature survey you might be in the sigma category, and you might find that system B is better. So in the case, we will have to also define measures that will quantify them. And we might need it to define multiple measures because users have different perspectives of looking at the results.
9:58:[MUSIC]
0:00:[SOUND] This lecture is about the basic measures for evaluation of text retrieval systems. In this lecture, we're going to discuss how we design basic measures to quantitatively compare two retrieval systems. This is a slide that you have seen earlier in the lecture where we talked about the Granville evaluation methodology. We can have a test faction that consists of queries, documents, and [INAUDIBLE]. We can then run two systems on these data sets to contradict the evaluator. Their performance. And we raise the question, about which set of results is better. Is system A better or is system B better? So let's now talk about how to accurately quantify their performance. Suppose we have a total of 10 relevant documents in the collection for this query. Now, the relevant judgments show on the right in [INAUDIBLE] obviously. And we have only seen 3 [INAUDIBLE] there, [INAUDIBLE] documents there. But, we can imagine there are other Random documents in judging for this query. So now, intuitively, we thought that system A is better because it did not have much noise. And in particular we have seen that among the three results, two of them are relevant but in system B, we have five results and only three of them are relevant. So intuitively it looks like system A is more accurate. And this infusion can be captured by a matching holder position, where we simply compute to what extent all the retrieval results are relevant. If you have 100% position, that would mean that all the retrieval documents are relevant. So in this case system A has a position of two out of three System B has some sweet hold of 5 and this shows that system A is better frequency. But we also talked about System B might be prefered by some other units would like to retrieve as many random documents as possible. So in that case we'll have to compare the number of relevant documents that they retrieve and there's another method called recall. This method uses the completeness of coverage of random documents In your retrieval result. So we just assume that there are ten relevant documents in the collection. And here we've got two of them, in system A. So the recall is 2 out of 10. Whereas System B has called a 3, so it's a 3 out of 10. Now we can see by recall system B is better. And these two measures turn out to be the very basic of measures for evaluating search engine. And they are very important because they are also widely used in many other test evaluation problems. For example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported and for all kinds of tasks.
3:35:Okay so, now let's define these two measures more precisely. And these measures are to evaluate a set of retrieved documents, so that means we are considering that approximation of the set of relevant documents.
3:50:We can distinguish 4 cases depending on the situation of the documents. A document can be retrieved or not retrieved, right? Because we are talking about a set of results.
4:02:A document can be also relevant or not relevant depending on whether the user thinks this is a useful document.
4:11:So we can now have counts of documents in. Each of the four categories again have a represent the number of documents that have been retrieved and relevant. B for documents that are not retrieved but rather etc.
4:31:No with this table then we can define precision.
4:36:As the ratio of the relevant retrieved documents A to the total of relevant retrieved documents.
4:48:So, this is just A divided by The sum of a and c. The sum of this column.
4:56:Singularly recall is defined by dividing a by the sum of a and b. So that's again to divide a by. The sum of the row instead of the column. All right, so we can see precision and recall is all focused on looking at the a,
5:16:that's the number of retrieved relevant documents. But we're going to use different denominators.
5:23:Okay, so what would be an ideal result. Well, you can easily see being the ideal case would have precision and recall oil to be 1.0. That means We have got 1% of all the Relevant documents in our results, and all of the results that we returned all Relevant. At least there's no single Not Relevant document returned.
5:48:In reality, however, high recall tends to be associated with low precision. And you can imagine why that's the case. As you go down the to try to get as many random documents as possible, you tend to encounter a lot of documents, so the precision has to go down. Note that this set can also be defined by a cut off. In the rest of this, that's why although these two measures are defined for retrieve the documents, they are actually very useful for evaluating a rank list. They are the fundamental measures in task retrieval and many other tasks. We often are interested in The precision at ten documents for web search. This means we look at how many documents among the top ten results are actually relevant. Now, this is a very meaningful measure, because it tells us how many relevant documents a user can expect to see On the first page of where they typically show ten results.
6:50:So precision and recall are the basic matches and we need to use them to further evaluate a search engine, but they are the Building blocks.
7:03:We just said that there tends to be a trailoff between precision and recall, so naturally it would be interesting to combine them. And here's one method that's often used, called F-measure And it's a [INAUDIBLE] mean of precision and recall as defined on this slide.
7:22:So, you can see at first, compute the.
7:29:Inverse of R and P here, and then it would interpret the 2 by using coefficients depending on parameter beta.
7:42:And after some transformation you can easily see it would be of this form.
7:49:And in any case it just becomes an agent of precision and recall, and beta is a parameter, that's often set to 1. It can control the emphasis on precision or recall always set beta to 1 We end up having a special case of F-Measure, often called F1. This is a popular measure that's often used as a combined precision and recall. And the formula looks very simple.
8:16:It's just this, here.
8:20:Now it's easy to see that if you have a Larger precision, or larger recall than f measure would be high. But, what's interesting is that the trade off between precision and recall is captured an interesting way in f1. So, in order to understand that, we
8:42:can first look at the natural Why not just the combining and using the symbol arithmetically as efficient here? That would be likely the most natural way of combining them So what do you think?
9:01:If you want to think more, you can pause the video.
9:07:So why is this not as good as F1?
9:13:Or what's the problem with this?
9:18:Now, if you think about the arithmetic mean, you can see this is the sum of multiple terms. In this case, it's the sum of precision and recall. In the case of a sum, the total value tends to be dominated by the large values. that means if you have a very high P or very high R then you really don't care about whether the other value is low so the whole sum would be high. Now this is not desirable because one can easily have a perfect recall. We have perfect recall easily. Can we imagine how?
9:59:It's probably very easy to imagine that we simply retrieve all the documents in the collection and then we have a perfect recall.
10:07:And this will give us 0.5 as the average. But such results are clearly not very useful for the users even though the average using this formula would be relevantly high.
10:21:In contrast you can see F 1 would reward a case where precision and recall are roughly That seminar, so it would a case where you had extremely high value for one of them.
10:35:So this means f one encodes a different trade off between that. Now this example shows actually a very important. Methodology here. But when you try to solve a problem you might naturally think of one solution, let's say in this it's this error mechanism.
10:53:But it's important not to settle on this source. It's important to think whether you have other ways to combine that.
11:02:And once you think about the multiple variance It's important to analyze their difference, and then think about which one makes more sense. In this case, if you think more carefully, you will think that F1 probably makes more sense. Than the simple. Although in other cases there may be different results. But in this case the seems not reasonable. But if you don't pay attention to these subtle differences you might just take a easy way to combine them and then go ahead with it. And here later, you will find that, the measure doesn't seem to work well. All right. So this methodology is actually very important in general, in solving problems. Try to think about the best solution. Try to understand the problem very well, and then know why you needed this measure, and why you need to combine precision and recall. And then use that to guide you in finding a good way to solve the problem.
12:03:To summarize, we talked about precision which addresses the question are there retrievable results all relevant? We also talk about the Recall. Which addresses the question, have all of the relevant documents been retrieved. These two, are the two, basic matches in text and retrieval in. They are used for many other tasks, as well. We talk about F measure as a way to combine Precision Precision and recall.
12:29:We also talked about the tradeoff between precision and recall. And this turns out to depend on the user's search tasks and we'll discuss this point more in a later lecture. [MUSIC]
0:00:[MUSIC]
0:07:This lecture is about, how we can evaluate a ranked list?
0:13:In this lecture, we will continue the discussion of evaluation. In particular, we are going to look at, how we can evaluate a ranked list of results.
0:24:In the previous lecture, we talked about, precision-recall. These are the two basic measures for, quantitatively measuring the performance of a search result.
0:40:But, as we talked about, ranking, before, we framed that the text of retrieval problem, as a ranking problem.
0:50:So, we also need to evaluate the, the quality of a ranked list.
0:56:How can we use precision-recall to evaluate, a ranked list? Well, naturally, we have to look after the precision-recall at different, cut-offs. Because in the end, the approximation of relevant documents, set, given by a ranked list, is determined by where the user stops browsing. Right? If we assume the user, securely browses, the list of results, the user would, stop at some point, and that point would determine the set. And then, that's the most important, cut-off, that we have to consider, when we compute the precision-recall. Without knowing where exactly user would stop, then we have to consider, all the positions where the user could stop. So, let's look at these positions. Look at this slide, and then, let's look at the, what if the user stops at the, the first document? What's the precision-recall at this point? What do you think?
1:56:Well, it's easy to see, that this document is So, the precision is one out of one. We have, got one document, and that's relevent. What about the recall? Well, note that, we're assuming that, there are ten relevant documents, for this query in the collection, so, it's one out of ten.
2:16:What if the user stops at the second position?
2:19:Top two.
2:21:Well, the precision is the same, 100%, two out of two. And, the record is two out of ten.
2:28:What if the user stops at the third position? Well, this is interesting, because in this case, we have not got any, additional relevant document, so, the record does not change.
2:41:But the precision is lower, because we've got number [INAUDIBLE] so, what's exactly the precision?
2:49:Well, it's two out of three, right? And, recall is the same, two out of ten. So, when would see another point, where the recall would be different? Now, if you look down the list, well, it won't happen until, we have, seeing another relevant document. In this case D5, at that point, the, the recall is increased through three out of ten, and, the precision is three out of five.
3:15:So, you can see, if we keep doing this, we can also get to D8. And then, we will have a precision of four out of eight, because there are eight documents, and four of them are relevant. And, the recall is a four out of ten.
3:29:Now, when can we get, a recall of five out of ten? Well, in this list, we don't have it, so, we have to go down on the list. We don't know, where it is? But, as convenience, we often assume that, the precision is zero,
3:47:at all the, the othe, the precision are zero at all the other levels of recall, that are beyond the search results. So, of course, this is a pessimistic assumption, the actual position would be higher, but we make, make this assumption,
4:05:in order to, have an easy way to, compute another measure called Average Precision, that we will discuss later.
4:14:Now, I should also say, now, here you see, we make these assumptions that are clearly not, accurate.
4:22:But, this is okay, for the purpose of comparing to, text methods. And, this is for the relative comparison, so, it's okay, if the actual measure, or actual, actual number deviates a little bit, from the true number. As long as the deviation, is not biased toward any particular retrieval method, we are okay. We can still, accurately tell which method works better. And, this is important point, to keep in mind. When you compare different algorithms, the key's to avoid any bias toward each method. And, as long as, you can avoid that. It's okay, for you to do transformation of these measures anyway, so, you can preserve the order.
5:09:Okay, so, we'll just talk about, we can get a lot of precision-recall numbers at different positions. So, now, you can imagine, we can plot a curve. And, this just shows on the, x-axis, we show the recalls.
5:23:And, on the y-axis, we show the precision. So, the precision line was marked as .1, .2, .3, and, 1.0. Right? So, this is, the different, levels of recall. And,, the y-axis also has, different amounts, that's for precision.
5:45:So, we plot the, these, precision-recall numbers, that we have got, as points on this picture. Now, we can further, and link these points to form a curve. As you'll see, we assumed all the other, precision as the high-level recalls, be zero. And, that's why, they are down here, so, they are all zero. And this, the actual curve probably will be something like this, but, as we just discussed, it, it doesn't matter that much, for comparing two methods.
6:20:because this would be, underestimated, for all the method.
6:25:Okay, so, now that we, have this precision-recall curve, how can we compare ranked to back list? All right, so, that means, we have to compare two PR curves.
6:38:And here, we show, two cases. Where system A is showing red, system B is showing blue, there's crosses.
6:48:All right, so, which one is better? I hope you can see, where system A is clearly better. Why? Because, for the same level of recall,
6:58:see same level of recall here, and you can see, the precision point by system A is better, system B. So, there's no question. In here, you can imagine, what does the code look like, for ideal search system? Well, it has to have perfect, precision at all the recall points, so, it has to be this line. That would be the ideal system. In general, the higher the curve is, the better, right? The problem is that, we might see a case like this. This actually happens often. Like, the two curves cross each other.
7:32:Now, in this case, which one is better?
7:35:What do you think?
7:38:Now, this is a real problem, that you actually, might have face. Suppose, you build a search engine, and you have a old algorithm, that's shown here in blue, or system B. And, you have come up with a new idea. And, you test it. And, the results are shown in red, curve A.
7:59:Now, your question is, is your new method better than the old method?
8:05:Or more, practically, do you have to replace the algorithm that you're already using, your, in your search engine, with another, new algorithm? So, should we use system, method A, to replace method B? This is going to be a real decision, that you to have to make. If you make the replacement, the search engine would behave like system A here, whereas, if you don't do that, it will be like a system B. So, what do you do?
8:36:Now, if you want to spend more time to think about this, pause the video. And, it's actually very useful to think about that. As I said, it's a real decision that you have to make, if you are building your own search engine, or if you're working, for a company that, cares about the search.
8:52:Now, if you have thought about this for a moment, you might realize that, well, in this case, it's hard to say. Now, some users might like a system A, some users might like, like system B. So, what's the difference here? Well, the difference is just that, you know, in the, low level of recall, in this region, system B is better. There's a higher precision. But in high recall region, system A is better.
9:20:Now, so, that also means, it depends on whether the user cares about the high recall, or low recall, but high precision. You can imagine, if someone is just going to check out, what's happening today, and want to find out something relevant in the news.
9:34:Well, which one is better? What do you think?
9:38:In this case, clearly, system B is better, because the user is unlikely examining a lot of results. The user doesn't care about high recall.
9:47:On the other hand, if you think about a case, where a user is doing you are, starting a problem. You want to find, whether your idea ha, has been started before. In that case, you emphasize high recall. So, you want to see, as many relevant documents as possible. Therefore, you might, favor, system A. So, that means, which one is better? That actually depends on users, and more precisely, users task.
10:19:So, this means, you may not necessarily be able to come up with one number,
10:25:that would accurately depict the performance.
10:29:You have to look at the overall picture. Yet, as I said, when you have a practical decision to make, whether you replace ours with another, then you may have to actually come up with a single number, to quantify each, method. Or, when we compare many different methods in research, ideally, we have one number to compare, them with, so, that we can easily make a lot of comparisons. So, for all these reasons, it is desirable to have one, single number to match it up. So, how do we do that? And, that, needs a number to summarize the range. So, here again it's the precision-recall curve, right? And, one way to summarize this whole ranked, list, for this whole curve, is look at the area underneath the curve.
11:19:Right? So, this is one way to measure that. There are other ways to measure that, but, it just turns out that,,
11:26:this particular way of matching it has been very, popular, and has been used, since a long time ago for text And, this is, basically, in this way, and it's called the average precision. Basically, we're going to take a, a look at the, every different, recall point.
11:47:And then, look out for the precision. So, we know, you know, this is one precision. And, this is another, with, different recall. Now, this, we don't count to this one, because the recall level is the same, and we're going to, look at the, this number, and that's precision at a different recall level et cetera. So, we have all these, you know, added up. These are the precisions at the different points, corresponding to retrieving the first relevant document, the second, and then, the third, that follows, et cetera. Now, we missed the many relevant documents, so, in all of those cases, we just, assume, that they have zero precisions.
12:33:And then, finally, we take the average. So, we divide it by ten, and which is the total number of relevant documents in the collection.
12:41:Note that here, we're not dividing this sum by four. Which is a number retrieved relevant documents. Now, imagine, if I divide by four, what would happen?
12:54:Now, think about this, for a moment.
12:57:It's a common mistake that people, sometimes, overlook.
13:02:Right, so, if we, we divide this by four, it's actually not very good. In fact, that you are favoring a system, that would retrieve very few random documents, as in that case, the denominator would be very small. So, this would be, not a good matching. So, note that this denomina, denominator is ten, the total number of relevant documents. And, this will basically ,compute the area, and the needs occur. And, this is the standard method, used for evaluating a ranked list.
13:41:Note that, it actually combines recall and, precision. But first, you know, we have precision numbers here, but secondly, we also consider recall, because if missed many, there would be many zeros here. All right, so, it combines precision and recall. And furthermore, you can see this measure is sensitive to a small change of a position of a relevant document. Let's say, if I move this relevant document up a little bit, now, it would increase this means, this average precision. Whereas, if I move any relevant document, down, let's say, I move this relevant document down, then it would decrease, uh,the average precision. So, this is a very good, because it's a very sensitive to the ranking of every relevant document. It can tell, small differences between two ranked lists. And, that is what we want, sometimes one algorithm only works slightly better than another. And, we want to see this difference. In contrast, if we look at the precision at the ten documents. If we look at this, this whole set, well, what, what's the precision, what do you think? Well, it's easy to see, that's a four out of ten, right? So, that precision is very meaningful, because it tells us, what user would see? So, that's pretty useful, right? So, it's a meaningful measure, from a users perspective. But, if we use this measure to compare systems, it wouldn't be good, because it wouldn't be sensitive to where these four relevant documents are ranked. If I move them around the precision at ten, still, the same. Right. So, this is not a good measure for comparing different algorithms. In contrast, the average precision is a much better measure. It can tell the difference of, different, a difference in ranked list in, subtle ways. [MUSIC]
0:00:[SOUND]
0:11:So average precision is computer for just one. one query. But we generally experiment with many different queries and this is to avoid the variance across queries. Depending on the queries you use you might make different conclusions. Right, so it's better then using more queries.
0:33:If you use more queries then, you will also have to take the average of the average precision over all these queries.
0:41:So how can we do that?
0:43:Well, you can naturally. Think of just doing arithmetic mean as we
0:50:always tend to, to think in, in this way. So, this would give us what's called a "Mean Average Position", or MAP. In this case, we take arithmetic mean of all the average precisions over several queries or topics.
1:09:But as I just mentioned in another lecture, is this good?
1:15:We call that. We talked about the different ways of combining precision and recall. And we conclude that the arithmetic mean is not as good as the MAP measure. But here it's the same. We can also think about the alternative ways of aggregating the numbers. Don't just automatically assume that, though. Let's just also take the arithmetic mean of the average position over these queries. Let's think about what's the best way of aggregating them. If you think about the different ways, naturally you will, probably be able to think about another way, which is geometric mean.
1:51:And we call this kind of average a gMAP.
1:55:This is another way. So now, once you think about the two different ways. Of doing the same thing. The natural question to ask is, which one is better? So.
2:05:So, do you use MAP or gMAP?
2:09:Again, that's important question. Imagine you are again testing a new algorithm in, by comparing the ways your old algorithms made the search engine.
2:18:Now you tested multiple topics. Now you've got the average precision for these topics. Now you are thinking of looking at the overall performance. You have to take the average.
2:30:But which, which strategy would you use?
2:34:Now first, you should also think about the question, well did it make a difference? Can you think of scenarios where using one of them would make a difference? That is they would give different rankings of those methods. And that also means depending on the way you average or detect the. Average of these average positions.
2:55:You will get different conclusions. This makes the question becoming even more important.
3:01:Right? So, which one would you use?
3:05:Well again, if you look at the difference between these. Different ways of aggregating the average position. You'll realize in arithmetic mean, the sum is dominating by large values. So what does large value here mean? It means the query is relatively easy. You can have a high pres, average position.
3:25:Whereas gMAP tends to be affected more by low values.
3:30:And those are the queries that don't have good performance. The average precision is low.
3:37:So if you think about the, improving the search engine for those difficult queries, then gMAP would be preferred, right?
3:47:On the other hand, if you just want to. Have improved a lot.
3:52:Over all the kinds of queries or particular popular queries that might be easy and you want to make the perfect and maybe MAP would be then preferred. So again, the answer depends on your users, your users tasks and their pref, their preferences.
4:08:So the point that here is to think about the multiple ways to solve the same problem, and then compare them, and think carefully about the differences. And which one makes more sense. Often, when one of them might make sense in one situation and another might make more sense in a different situation. So it's important to pick out under what situations one is preferred.
4:35:As a special case of the mean average position, we can also think about the case where there was precisely one rank in the document. And this happens often, for example, in what's called a known item search. Where you know a target page, let's say you have to find Amazon, homepage. You have one relevant document there, and you hope to find it. That's call a "known item search". In that case, there's precisely one relevant document. Or in another application, like a question and answering, maybe there's only one answer. Are there. So if you rank the answers, then your goal is to rank that one particular answer on top, right? So in this case, you can easily verify the average position, will basically boil down to reciprocal rank. That is, 1 over r where r is the rank position of that single relevant document. So if that document is ranked on the very top or is 1, and then it's 1 for reciprocal rank. If it's ranked at the, the second, then it's 1 over 2. Et cetera.
5:41:And then we can also take a, a average of all these average precision or reciprocal rank over a set of topics, and that would give us something called a mean reciprocal rank. It's a very popular measure. For no item search or, you know, an problem where you have just one relevant item.
6:03:Now again here, you can see this r actually is meaningful here. And this r is basically indicating how much effort a user would have to make in order to find that relevant document. If it's ranked on the top it's low effort that you have to make, or little effort. But if it's ranked at 100 then you actually have to,
6:27:read presumably 100 documents in order to find it. So, in this sense r is also a meaningful measure and the reciprocal rank will take the reciprocal of r, instead of using r directly.
6:42:So my natural question here is why not simply using r? I imagine if you were to design a ratio to, measure the performance of a random system, when there is only one relevant item.
6:55:You might have thought about using r directly as the measure. After all, that measures the user's effort, right? But, think about if you take a average of this over a large number of topics.
7:12:Again it would make a difference. Right, for one single topic, using r or using 1 over r wouldn't make any difference. It's the same. Larger r with corresponds to a small 1 over r, right?
7:26:But the difference would only show when, show up when you have many topics. So again, think about the average of Mean Reciprocal Rank versus average of just r. What's the difference? Do you see any difference? And would, would this difference change the oath of systems. In our conclusion.
7:49:And this, it turns out that, there is actually a big difference, and if you think about it, if you want to think about it and then, yourself, then pause the video.
7:59:Basically, the difference is, if you take some of our directory, then. Again it will be dominated by large values of r. So what are those values? Those are basically large values that indicate that lower ranked results. That means the relevant items rank very low down on the list. And the sum that's also the average that would then be dominated by. Where those relevant documents are ranked in, in ,in, in the lower portion of the ranked. But from a users perspective we care more about the highly ranked documents. So by taking this transformation by using reciprocal rank.
8:40:Here we emphasize more on the difference on the top. You know, think about the difference between 1 and the 2, it would make a big difference, in 1 over r, but think about the 100, and 1, and where and when won't make much difference if you use this. But if you use this there will be a big difference in 100 and let's say 1,000, right. So this is not the desirable.
9:06:On the other hand, a 1 and 2 won't make much difference. So this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense.
9:17:So to summarize, we showed that the precision-recall curve. Can characterize the overall accuracy of a ranked list. And we emphasized that the actual utility of a ranked list depends on how many top ranked results a user would actually examine. Some users will examine more. Than others. An average person uses a standard measure for comparing two ranking methods. It combines precision and recall and it's sensitive to the rank of every random document. [MUSIC]
0:00:[MUSIC]
0:07:This lecture is about how to evaluate the text retrieval system when we have multiple levels of judgements. In this lecture, we will continue the discussion of evaluation. We're going to look at how to evaluate a text retrieval system, when we have multiple levels of judgements.
0:27:So far we have talked about the binary judgements, that means a document is judged as being relevant or not relevant.
0:35:But earlier, we also talk about the relevance as a medal of degrees. So we often can distinguish very high relevant documents, those are very useful documents, from moderately relevant documents. They are okay, they are useful perhaps. And further from now, we're adding the documents, those are not useful.
0:57:So imagine you can have ratings for these pages. Then, you would have multiple levels of ratings. For example, here I show example of three levels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant, and 1 for non-relevant. Now, how do we evaluate the search engine system using these judgements? Obvious that the map doesn't work, average of precision doesn't work, precision, and recall doesn't work, because they rely on binary judgements. So let's look at some top ranked results when using these judgements. Imagine the user would be mostly care about the top ten results here.
1:43:And we marked the rating levels, or relevance levels, for these documents as shown here, 3, 2, 1, 1, 3, etcetera. And we call these gain. And the reason why we call it the gain is because the measure that we are infusing is called the NDCG normalized or accumulated gain.
2:10:So this gain, basically, can measure how much a gain of random information a user can obtain by looking at each document, right? So looking at the first document, the user can gain 3 points. Looking at the non-relevant document user would only gain 1 point.
2:29:Looking at the moderator or marginally relevant, document the user would get 2 points, etcetera. So, this gain to each of the measures is a utility of the document from a user's perspective. Of course, if we assume the user stops at the 10 documents and we're looking at the cutoff at 10, we can look at the total gain of the user. And what's that? Well, that's simply the sum of these, and we call it the Cumulative Gain. So if the user stops after the position 1, that's just a 3. If the user looks at another document, that's a 3+2. If the user looks at the more documents, then the cumulative gain is more. Of course this is at the cost of spending more time to examine the list. So cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents. Now, in NDCG, we also have another letter here, D, discounted cumulative gain.
3:29:So, why do we want to do discounting? Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents. So for example, looking at this sum here, and we only know there is 1 highly relevant document, 1 marginally relevant document, 2 non-relevant documents. We don't really care where they are ranked. Ideally, we want these two to be ranked on the top which is the case here.
4:03:But how can we capture that intuition? Well we have to say, well this is 3 here is not as good as this 3 on the top. And that means the contribution of the gain from different positions has to be weighted by their position. And this is the idea of discounting, basically. So we're going to to say, well, the first one does not need to be discounted because the user can be assumed that will always see this document. But the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it. So we divide this gain by a weight based on the position. So log of 2, 2 is the rank position of this document. And when we go to the third position, we discounted even more, because the normalizer is log of 3, and so on and so forth. So when we take such a sum that a lower ranked document would not contribute that much as a highly ranked document. So that means if you, for example, switch the position of this, let's say this position, and this one, and then you would get more discount if you put, for example very relevant document here as opposed to here. Imagine if you put the 3 here, then it would have to be discounted. So it's not as good as if you we would put the 3 here. So this is the idea of discounting.
5:37:Okay, so now at this point that we have got a discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgements.
5:51:So are we happy with this? Well, we can use this to rank systems. Now, we still need to do a little bit more in order to make this measure comparable across different topics. And this is the last step, and by the way, here we just show the DCG at 10, so this is the total sum of DCG, all these 10 documents. So the last step is called N, normalization. And if we do that, then we'll get the normalized DCG. So how do we do that? Well, the idea here is we're going to normalize DCG by the ideal DCG at the same cutoff. What is the ideal DCG? Well, this is the DCG of an ideal ranking. So imagine if we have 9 documents in the whole collection rated 3 here. And that means in total we have 9 documents rated 3.
6:53:Then our ideal rank lister would have put all these 9 documents on the very top. So all these would have to be 3 and then this would be followed by a 2 here. Because that's the best we could do after we have run out of the 3. But all these positions would be 3. Right? So this would our ideal ranked list.
7:18:And then we had computed the DCG for this ideal rank list.
7:23:So this would be given by this formula that you see here. And so this ideal DCG would then be used as the normalizer DCG. So here. And this idea of DCG would be used as a normalizer. So you can imagine now, normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic. Now why do we want to do this? Well, by doing this we'll map the DCG values into a range of 0 through 1.
7:57:So the best value, or the highest value, for every query would be 1. That's when your rank list is, in fact, the ideal list but otherwise, in general, you will be lower than one.
8:13:Now, what if we don't do that? Well, you can see, this transformation, or this normalization, doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG. The difference however is when we have multiple topics. Because if we don't do normalization, different topics will have different scales of DCG.
8:46:For a topic like this one, we have 9 highly relevant documents, the DCG can get really high, but imagine in another case, there are only two very relevant documents in total in the whole collection. Then the highest DCG that any system could achieve for such a topic would not be very high. So again, we face the problem of different scales of DCG values. When we take an average, we don't want the average to be dominated by those high values. Those are, again, easy queries. So, by doing the normalization, we can have avoided the problem, making all the queries contribute to equal to the average. So, this is a idea of NDCG, it's used for measuring a rank list based on multiple level of relevance judgements.
9:42:In a more general way this is basically a measure that can be applied to any ranked task with multiple level of judgements. And the scale of the judgements can be multiple, can be more than binary not only more than binary they can be much multiple levels like 1, 0, 5 or even more depending on your application. And the main idea of this measure, just to summarize, is to measure the total utility of the top k documents. So you always choose a cutoff and then you measure the total utility. And it would discount the contribution from a lowly ranked document. And then finally, it would do normalization to ensure comparability across queries. [MUSIC]
0:00:[SOUND]. This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems.
0:14:In this lecture, we will continue the discussion of evaluation. We'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems.
0:25:So, in order to create the test collection, we have to create a set of queries. A set of documents and a set of relevance judgments.
0:35:It turns out that each is actually challenging to create. First, the documents and queries must be representative. They must represent the real queries and real documents that the users handle.
0:48:And we also have to use many queries and many documents in order to avoid a bias of conclusions.
0:56:For the matching of relevant documents with the queries. We also need to ensure that there exists a lot of relevant documents for each query. If a query has only one, that's a relevant option we can actually then. It's not very informative to compare different methods using such a query because there's not that much room for us to see difference. So ideally, there should be more relevant documents in the clatch but yet the queries also should represent the real queries that we care about.
1:31:In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries. Yet, minimizing human and fault, because we have to use human labor to label these documents. It's very labor intensive. And as a result, it's impossible to actually label all the documents for all the queries, especially considering a giant data set like the web.
1:58:So this is actually a major challenge, it's a very difficult challenge. For measures, it's also challenging, because we want measures that would accurately reflect the perceived utility of users. We have to consider carefully what the users care about. And then design measures to measure that. If your measure is not measuring the right thing, then your conclusion would be misled. So it's very important.
2:26:So we're going to talk about a couple of issues here. One is the statistical significance test. And this also is a reason why we have to use a lot of queries. And the question here is how sure can you be that observe the difference doesn't simply result from the particular queries you choose. So here are some sample results of average position for System A and System B into different experiments. And you can see in the bottom, we have mean average of position. So the mean, if you look at the mean average of position, the mean average of positions are exactly the same in both experiments, right? So you can see this is 0.20, this is 0.40 for System B. And again here it's also 0.20 and 0.40, so they are identical. Yet, if you look at these exact average positions for different queries. If you look at these numbers in detail, you would realize that in one case, you would feel that you can trust the conclusion here given by the average.
3:36:In the another case, in the other case, you will feel that, well, I'm not sure. So, why don't you take a look at all these numbers for a moment, pause the media. So, if you look at the average, the mean average of position, we can easily, say that well, System B is better, right? So, after all it's 0.40 and this is twice as much as 0.20, so that's a better performance. But if you look at these two experiments, look at the detailed results.
4:11:You will see that, we've been more confident to say that, in the case one, in experiment one. In this case. Because these numbers seem to be consistently better for System B.
4:25:Whereas in Experiment 2, we're not sure because looking at some results like this, after System A is better and this is another case System A is better.
4:39:But yet if we look at only average, System B is better.
4:45:So, what do you think?
4:49:How reliable is our conclusion, if we only look at the average?
4:55:Now in this case, intuitively, we feel Experiment 1 is more reliable.
5:01:But how can we quantitate the answer to this question? And this is why we need to do statistical significance test.
5:09:So, the idea of the statistical significance test is basically to assess the variants across these different queries. If there is a big variance, that means the results could fluctuate a lot according to different queries. Then we should believe that, unless you have used a lot of queries, the results might change if we use another set of queries. Right, so this is then not so if you have c high variance then it's not very reliable.
5:43:So let's look at these results again in the second case. So, here we show two different ways to compare them. One is a sign test where we just look at the sign. If System B is better than System A, we have a plus sign. When System A is better we have a minus sign, etc. Using this case, if you see this, well, there are seven cases. We actually have four cases where System B is better. But three cases of System A is better, intuitively, this is almost like a random results, right? So if you just take a random sample of you flip seven coins and if you use plus to denote the head and minus to denote the tail and that could easily be the results of just randomly flipping these seven coins. So, the fact that the average is larger doesn't tell us anything. We can't reliably conclude that. And this can be quantitatively measured by a p value. And that basically means
6:49:the probability that this result is in fact from a random fluctuation. In this case, probability is 1.0. It means it surely is a random fluctuation.
7:01:Now in Willcoxan test, it's a non-parametric test, and we would be not only looking at the signs, we'll be also looking at the magnitude of the difference. But we can draw a similar conclusion, where you say it's very likely to be from random. To illustrate this, let's think about that such a distribution. And this is called a now distribution. We assume that the mean is zero here. Lets say we started with assumption that there's no difference between the two systems. But we assume that because of random fluctuations depending on the queries, we might observe a difference. So the actual difference might be on the left side here or on the right side here, right?
7:43:So, and this curve kind of shows the probability that we will actually observe values that are deviating from zero here.
7:53:Now, so if we look at this picture then, we see that
8:01:if a difference is observed here, then the chance is very high that this is in fact a random observation, right? We can define a region of likely observation because of random fluctuation and this is that 95% of all the outcomes. And in this then the observed may still be from random fluctuation.
8:28:But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation. All right, so there's a very small probability that you are observe such a difference just because of random fluctuation.
8:48:So in that case, we can then conclude the difference must be real. So System B is indeed better.
8:56:So this is the idea of Statical Significance Test. The takeaway message here is that you have to use many queries to avoid jumping into a conclusion. As in this case, to say System B is better.
9:09:There are many different ways of doing this Statistical Significance Test.
9:15:So now, let's talk about the other problem of making judgments and, as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set. So the question is, if we can afford judging all the documents in the collection, which is subset should we judge?
9:35:And the solution here is Pooling. And this is a strategy that has been used in many cases to solve this problem.
9:46:So the idea of Pooling is the following. We would first choose a diverse set of ranking methods. These are Text Retrieval systems.
9:57:And we hope these methods can help us nominate like the relevant documents. So the goal is to pick out the relevant documents. We want to make judgements on relevant documents because those are the most useful documents from users perspectives. So then we're going to have each to return top-K documents.
10:17:The K can vary from systems. But the point is to ask them to suggest the most likely relevant documents.
10:25:And then we simply combine all these top-K sets to form a pool of documents for human assessors. To judge, so imagine you have many systems each were ten k documents. We take the top-K documents, and we form a union. Now, of course, there are many documents that are duplicated because many systems might have retrieved the same random documents. So there will be some duplicate documents.
10:56:And there are also unique documents that are only returned by one system. So the idea of having diverse set of ranking methods is to ensure the pool is broad. And can include as many possible relevant documents as possible.
11:12:And then, the users would, human assessors would make complete the judgments on this data set, this pool. And the other unjudged the documents are usually just assumed to be non relevant. Now if the pool is large enough, this assumption is okay.
11:32:But if the pool is not very large, this actually has to be reconsidered. And we might use other strategies to deal with them and there are indeed other methods to handle such cases. And such a strategy is generally okay for comparing systems that contribute to the pool. That means if you participate in contributing to the pool, then it's unlikely that it would penalize your system because the problematic documents have all been judged.
12:04:However, this is problematic for evaluating a new system that may have not contributed to the pool. In this case, a new system might be penalized because it might have nominated some read only documents that have not been judged. So those documents might be assumed to be non relevant. That's unfair. So to summarize the whole part of textual evaluation, it's extremely important. Because the problem is the empirically defined problem, if we
12:38:don't rely on users, there's no way to tell whether one method works better.
12:43:If we have in the property experiment design, we might misguide our research or applications. And we might just draw wrong conclusions. And we have seen this is in some of our discussions. So make sure to get it right for your research or application.
13:00:The main methodology is the Cranfield evaluation methodology. And they are the main paradigm used in all kinds of empirical evaluation tasks, not just a search engine variation. Map and nDCG are the two main measures that you should definitely know about and they are appropriate for comparing ranking algorithms. You will see them often in research papers. Precision at 10 documents is easier to interpret from user's perspective. So that's also often useful.
13:30:What's not covered is some other evaluation strategy like A-B Test. Where the system would mix two, the results of two methods, randomly. And then would show the mixed results to users. Of course, the users don't see which result, from which method. The users would judge those results or click on those documents in a search engine application. In this case then, the search engine can check or click the documents and see if one method has contributed more through the click the documents. If the user tends to click on one, the results from one method,
14:13:then it suggests that message may be better. So this is what leverages the real users of a search engine to do evaluation. It's called A-B Test and it's a strategy that is often used by the modern search engines or commercial search engines. Another way to evaluate IR or textual retrieval is user studies and we haven't covered that. I've put some references here that you can look at if you want to know more about that.
14:41:So, there are three additional readings here. These are three mini books about evaluation and they are all excellent in covering a broad review of Information Retrieval Evaluation. And it covers some of the things that we discussed, but they also have a lot of others to offer.
15:02:[MUSIC]
0:00:[SOUND] This lecture is about the Probabilistic Retrieval Model. In this lecture, we're going to continue the discussion of the Text Retrieval Methods. We're going to look at another kind of very different way to design ranking functions than the Vector Space Model that we discussed before.
0:32:In probabilistic models, we define the ranking function, based on the probability that this document is relevant to this query. In other words, we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observations from random variables.
1:00:Note that in the vector-based models, we assume they are vectors, but here we assume they are the data observed from random variables. And so, the problem of retrieval becomes to estimate the probability of relevance.
1:19:In this category of models, there are different variants. The classic probabilistic model has led to the BM25 retrieval function, which we discussed in in the vectors-based model because its a form is actually similar to a backwards space model.
1:35:In this lecture, we will discuss another sub class in this
1:41:P class called a language modeling approaches to retrieval. In particular, we're going to discuss the query likelihood retrieval model,
1:51:which is one of the most effective models in probabilistic models.
1:57:There was also another line called the divergence from randomness model which has led to the PL2 function,
2:06:it's also one of the most effective state of the art retrieval functions. In query likelihood, our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively, this probability just captures the following probability. And that is if a user likes document d, how likely would the user enter query q ,in order to retrieve document d? So we assume that the user likes d, because we have a relevance value here. And then we ask the question about how likely we'll see this particular query from this user?
2:54:So this is the basic idea. Now, to understand this idea, let's take a look at the general idea or the basic idea of Probabilistic Retrieval Models. So here, I listed some imagined relevance status values or relevance judgments of queries and documents. For example, in this line, it shows that q1 is a query that the user typed in. And d1 is a document that the user has seen. And 1 means the user thinks d1 is relevant to q1. So this R here can be also approximated by the click-through data that a search engine can collect by watching how you interacted with the search results. So in this case, let's say the user clicked on this document. So there's a 1 here.
3:50:Similarly, the user clicked on d2 also, so there is a 1 here. In other words, d2 is assumed to be relevant to q1.
4:00:On the other hand, d3 is non-relevant, there's a 0 here.
4:07:And d4 is non-relevant and then d5 is again, relevant, and so on and so forth. And this part, maybe, data collected from a different user. So this user typed in q1 and then found that the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typed in by the same user at different times. But d2 is also relevant, etc. And then here, we can see more data about other queries.
4:48:Now, we can imagine we have a lot of such data.
4:52:Now we can ask the question, how can we then estimate the probability of relevance?
5:00:So how can we compute this probability of relevance? Well, intuitively that just means if we look at all the entries where we see this particular d and this particular q, how likely we'll see a one on this other column. So basically that just means that we can just collect the counts.
5:19:We can first count how many times we have seen q and d as a pair in this table and then count how many times we actually have also seen 1 in the third column. And then, we just compute the ratio.
5:39:So let's take a look at some specific examples. Suppose we are trying to compute this probability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability.
6:07:Have you seen that, if we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the user has said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two and so on and so forth. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for d1, d2 and d3 for this query. And we can simply rank them based on these probabilities and so that's the basic idea probabilistic retrieval model. And you can see it makes a lot of sense, in this case, it's going to rank d2 above all the other documents. Because in all the cases, when you have c and q1 and d2, R = 1. The user clicked on this document. So this also should show that with a lot of click-through data, a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even with small amount of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user for typing this query.
7:47:Now, of course, the problems that we don't observe all the queries and all the documents and all the relevance values, right?
7:55:There would be a lot of unseen documents, in general, we have only collected the data from the documents that we have shown to the users. And there are even more unseen queries because you cannot predict what queries will be typed in by users. So obviously, this approach won't work if we apply it to unseen queries or unseen documents.
8:18:Nevertheless, this shows the basic idea of probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and unseen queries? Well, the solutions that we have to approximate in some way. So in this particular case called a query likelihood retrieval model, we just approximate this by another conditional probability. p(q given d, R=1). So in the condition part, we assume that the user likes the document because we have seen that the user clicked on this document.
8:56:And this part shows that we're interested in how likely the user would actually enter this query. How likely we will see this query in the same row. So note that here, we have made an interesting assumption here. Basically, we're going to do, assume that whether the user types in this query has something to do with whether user likes the document. In other words, we actually make the following assumption.
9:22:And that is a user formulates a query based on an imaginary relevant document. Where if you just look at this as conditional probability, it's not obvious we are making this assumption. So what I really meant is that to use this new conditional probability to help us score, then this new conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Otherwise we would be having similar problems as before, and by making this assumption, we have some way to bypass this big table, and try to just model how the user formulates the query, okay? So this is how you can simplify the general model so that we can derive a specific relevant function later. So let's look at how this model work for our example. And basically, what we are going to do in this case is to ask the following question. Which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query? So we ask this question and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind. Here you can see we've computed all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values, we can then rank these documents based on these values. So to summarize, the general idea of modern relevance in the proper risk model is to assume the we introduce a binary random variable R, here. And then, let the scoring function be defined based on this conditional probability. We also talked about approximating this by using the query likelihood.
11:22:And in this case we have a ranking function that's basically based on the probability of a query given the document. And this probability should be interpreted as the probability that a user who likes document d, would pose query q.
11:40:Now, the question of course is, how do we compute this conditional probability? At this in general has to do with how you compute the probability of text, because q is a text. And this has to do with a model called a Language Model. And these kind of models are proposed to model text.
12:02:So more specifically, we will be very interested in the following conditional probability as is shown in this here. If the user liked this document, how likely the user would pose this query. And in the next lecture we're going to do, giving introduction to language models that we can see how we can model text that was a probable risk model, in general. [MUSIC]
0:00:[SOUND]
0:07:[SOUND] This lecture is about the statistical language model. In this lecture, we're going to give an introduction to statistical language model. This has to do with how do you model text data with probabilistic models. So it's related to how we model query based on a document.
0:31:We're going to talk about what is a language model. And then we're going to talk about the simplest language model called the unigram language model, which also happens to be the most useful model for text retrieval. And finally, what this class will use is a language model.
0:47:What is a language model? Well, it's just a probability distribution over word sequences. So here, I'll show one.
0:55:This model gives the sequence Today is Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probability because it's non-grammatical.
1:11:You can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is most popular among these sentences. Imagine in the context of discussing apply the math, maybe the eigenvalue is positive, would have a higher probability. This means it can be used to represent the topic of a text.
1:42:The model can also be regarded as a probabilistic mechanism for generating text. And this is why it's also often called a generating model. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic system that can generate sequences of words. So, we can ask for a sequence, and it's to send for a sequence from the device if you want, and it might generate, for example, Today is Wednesday, but it could have generated any other sequences. So for example, there are many possibilities, right?
2:24:So in this sense, we can view our data as basically a sample observed from such a generating model. So, why is such a model useful? Well, it's mainly because it can quantify the uncertainties in natural language. Where do uncertainties come from? Well, one source is simply the ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don't have complete understanding, we lack all the knowledge to understand the language. In that case, there will be uncertainties as well. So let me show some examples of questions that we can answer with a language model that would have interesting applications in different ways. Given that we see John and feels, how likely will we see happy as opposed to habit as the next word in a sequence of words? Now, obviously, this would be very useful for speech recognition because happy and habit would have similar acoustic sound, acoustic signals. But, if we look at the language model, we know that John feels happy would be far more likely than John feels habit.
3:35:Another example, given that we observe baseball three times and game once in a news article, how likely is it about sports? This obviously is related to text categorization and information retrieval.
3:48:Also, given that a user is interested in sports news, how likely would the user use baseball in a query? Now, this is clearly related to the query likelihood that we discussed in the previous lecture.
4:02:So now, let's look at the simplest language model, called a unigram language model. In such a case, we assume that we generate a text by generating each word independently.
4:14:So this means the probability of a sequence of words would be then the product of the probability of each word. Now normally, they're not independent, right? So if you have single word in like a language, that would make it far more likely to observe model than if you haven't seen the language. So this assumption is not necessarily true, but we make this assumption to simplify the model.
4:41:So now the model has precisely N parameters, where N is vocabulary size. We have one probability for each word, and all these probabilities must sum to 1. So strictly speaking, we actually have N-1 parameters.
5:00:As I said, text can then be assumed to be assembled, drawn from this word distribution.
5:08:So for example, now we can ask the device or the model to stochastically generate the words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday, it now gives us just one word. And we can get all kinds of words. And we can assemble these words in a sequence. So that will still allow you to compute the probability of Today is Wednesday as the product of the three probabilities.
5:37:As you can see, even though we have not asked the model to generate the sequences, it actually allows us to compute the probability for all the sequences, but this model now only needs N parameters to characterize. That means if we specify all the probabilities for all the words, then the model's behavior is completely specified. Whereas if we don't make this assumption, we would have to specify probabilities for all kinds of combinations of words in sequences.
6:11:So by making this assumption, it makes it much easier to estimate these parameters. So let's see a specific example here.
6:19:Here I show two unigram language models with some probabilities. And these are high probability words that are shown on top.
6:29:The first one clearly suggests a topic of text mining, because the high probability was all related to this topic. The second one is more related to health.
6:39:Now we can ask the question, how likely were observe a particular text from each of these two models? Now suppose we sample words to form a document. Let's say we take the first distribution, would you like to sample words? What words do you think would be generated while making a text or maybe mining maybe another word? Even food, which has a very small probability, might still be able to show up.
7:03:But in general, high probability words will likely show up more often.
7:08:So we can imagine what general text of that looks like in text mining.
7:12:In fact, with small probability, you might be able to actually generate the actual text mining paper. Now, it will actually be meaningful, although the probability will be very, very small.
7:26:In an extreme case, you might imagine we might be able to generate a text mining paper that would be accepted by a major conference. And in that case, the probability would be even smaller. But it's a non-zero probability, if we assume none of the words have non-zero probability.
7:47:Similarly from the second topic, we can imagine we can generate a food nutrition paper. That doesn't mean we cannot generate this paper from text mining distribution.
7:59:We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mining.
8:10:So the point is that the keeping distribution,
8:13:we can talk about the probability of observing a certain kind of text. Some texts will have higher probabilities than others.
8:21:Now let's look at the problem in a different way. Suppose we now have available a particular document. In this case, many of the abstract or the text mining table, and we see these word counts here. The total number of words is 100. Now the question you ask here is an estimation question. We can ask the question which model, which one of these distribution has been used to generate this text, assuming that the text has been generated by assembling words from the distribution.
8:51:So what would be your guess?
8:54:What we have to decide are what probabilities text mining, etc., would have.
9:01:Suppose the view for a second, and try to think about your best guess.
9:09:If you're like a lot of people, you would have guessed that well, my best guess is text has a probability of 10 out of 100 because I've seen text 10 times, and there are in total 100 words. So we simply normalize these counts.
9:27:And that's in fact the word justified, and your intuition is consistent with mathematical derivation. And this is called the maximum likelihood estimator. In this estimator, we assume that the parameter settings of those that would give our observe the data the maximum probability. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller.
9:55:So you can see, this has a very simple formula. Basically, we just need to look at the count of a word in a document, and then divide it by the total number of words in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assign zero probabilities to unseen words. If we have an observed word, there will be no incentive to assign a non-zero probability using this approach. Why? Because that would take away probability mass for these observed words. And that obviously wouldn't maximize the probability of this particular observed text data. But one has still question whether this is our best estimate. Well, the answer depends on what kind of model you want to find, right? This estimator gives a best model based on this particular data. But if you are interested in a model that can explain the content of the full paper for this abstract, then you might have a second thought, right? So for thing, there should be other words in the body of that article, so they should not have zero probabilities, even though they're not observed in the abstract. So we're going to cover this a little bit more later in this class in the query likelihood model.
11:24:So let's take a look at some possible uses of these language models. One use is simply to use it to represent the topics. So here I show some general English background texts. We can use this text to estimate a language model, and the model might look like this.
11:42:Right, so on the top, we have those all common words, the, a, is, we, etc., and then we'll see some common words like these, and then some very, very rare words in the bottom. This is a background language model. It represents the frequency of words in English in general. This is the background model. Now let's look at another text, maybe this time, we'll look at the computer science research papers.
12:11:So we have a collection of computer science research papers, we do as mentioned again, we can just use the maximum likelihood estimator, where we simply normalize the frequencies.
12:20:Now in this case, we'll get the distribution that looks like this. On the top, it looks similar because these words occur everywhere, they are very common. But as we go down, we'll see words that are more related to computer science, computer software, text, etc. And so although here, we might also see these words, for example, computer, but we can imagine the probability here is much smaller than the probability here. And we will see many other words here that would be more common in general English. So you can see this distribution characterizes a topic of the corresponding text. We can look at even the smaller text.
13:03:So in this case, let's look at the text mining paper. Now if we do the same, we have another distribution, again the can be expected to occur in the top. The sooner we see text, mining, association, clustering, these words have relatively high probabilities. In contrast, in this distribution, the text has a relatively small probability. So this means, again, based on different text data, we can have a different model, and the model captures the topic. So we call this document the language model, and we call this collection language model. And later, you will see how they're used in the retrieval function.
13:47:But now, let's look at another use of this model. Can we statistically find what words are semantically related to computer?
13:56:Now how do we find such words? Well, our first thought is that let's take a look at the text that match computer. So we can take a look at all the documents that contain the word computer. Let's build a language model. We can see what words we see there. Well, not surprisingly, we see these common words on top as we always do. So in this case, this language model gives us the conditional probability of seeing the word in the context of computer. And these common words will naturally have high probabilities. But we also see the computer itself and software will have relatively high probabilities. But if we just use this model, we cannot just say all these words are semantically related to computer.
14:43:So ultimately, what we'd like to get rid of is these common words. How can we do that?
14:52:It turns out that it's possible to use language model to do that.
14:57:But I suggest you think about that. So how can we know what words are very common, so that we want to kind of get rid of them?
15:07:What model will tell us that? Well, maybe you can think about that. So the background language model precisely tells us this information. It tells us what was our common in general. So if we use this background model, we would know that these words are common words in general. So it's not surprising to observe them in the context of computer. Whereas computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software.
15:44:So then we can use these two models to somehow figure out the words that are related to computer. For example, we can simply take the ratio of these group probabilities and normalize the topic of language model by the probability of the word in the background language model. So if we do that, we take the ratio, we'll see that then on the top, computer is ranked, and then followed by software, program, all these words related to computer. Because they occur very frequently in the context of computer, but not frequently in the whole collection, whereas these common words will not have a high probability. In fact, they have a ratio about 1 down there because they are not really related to computer. By taking the sample of text that contains the computer, we don't really see more occurrences of that than in general.
16:40:So this shows that even with these simple language models, we can do some limited analysis of semantics.
16:48:So in this lecture, we talked about language model, which is basically a probability distribution over text. We talked about the simplest language model called unigram language model, which is also just a word distribution. We talked about the two uses of a language model. One is we represent the topic in a document, in a collection, or in general. The other is we discover word associations.
17:16:In the next lecture, we're going to talk about how language model can be used to design a retrieval function.
17:23:Here are two additional readings. The first is a textbook on statistical natural language processing.
17:30:The second is an article that has a survey of statistical language models with a lot of pointers to research work. [MUSIC]
0:00:[SOUND]
0:07:This lecture is about query likelihood, probabilistic retrieval model.
0:14:In this lecture, we continue the discussion of probabilistic retrieval model. In particular, we're going to talk about the query light holder retrieval function.
0:25:In the query light holder retrieval model, our idea is model. How like their user who likes a document with pose a particular query?
0:36:So in this case, you can imagine if a user likes this particular document about a presidential campaign news. Now we assume, the user would use this a document as a basis to impose a query to try and retrieve this document.
0:57:So again, imagine use a process that works as follows. Where we assume that the query is generated by assembling words from the document.
1:10:So for example, a user might pick a word like presidential, from this document and then use this as a query word.
1:20:And then the user would pick another word like campaign, and that would be the second query word.
1:27:Now this of course is an assumption that we have made about how a user would pose a query. Whether a user actually followed this process may be a different question, but this assumption has allowed us to formerly characterize this conditional probability.
1:46:And this allows us to also not rely on the big table that I showed you earlier
1:52:to use empirical data to estimate this probability.
1:56:And this is why we can use this idea then to further derive retrieval function that we can implement with the program language.
2:04:So as you see the assumption that we made here is each query word is independent of the sample. And also each word is basically obtained from the document.
2:20:So now let's see how this works exactly. Well, since we are completing a query likelihood
2:29:then the probability here is just the probability of this particular query, which is a sequence of words. And we make the assumption that each word is generated independently. So as a result, the probability of the query is just a product of the probability of each query word.
2:50:Now how do we compute the probability of each query word? Well, based on the assumption that a word is picked from the document that the user has in mind. Now we know the probability of each word is just the relative frequency of each word in the document. So for example, the probability of presidential given the document. Would be just the count of presidential document divided by the total number of words in the document or document s. So with these assumptions we now have actually a simple formula for retrieval. We can use this to rank our documents.
3:32:So does this model work? Let's take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here on the top.
3:45:So how do we score this document? Well, it's very simple. We just count how many times do we have seen presidential or how many times do we have seen campaigns, etc. And we see here 44, and we've seen presidential twice. So that's 2 over the length of document 4 multiplied by 1 over the length of document 4 for the probability of campaign. And similarly, we can get probabilities for the other two documents.
4:13:Now if you look at these numbers or these formulas for scoring all these documents, it seems to make sense. Because if we assume d3 and d4 have about the same length, then looks like a nominal rank d4 above d3 and which is above d2. And as we would expect, looks like it did captures a TF query state, and so this seems to work well. However, if we try a different query like this one, presidential campaign update then we might see a problem. Well what problem? Well think about the update. Now none of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining the word update would be what? Would be 0.
5:17:So that causes a problem, because it would cause all these documents to have zero probability of generating this query.
5:25:Now why it's fine to have zero probability for d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longer can distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about what has caused this problem?
5:52:So we have to examine what assumptions have been made, as we derive this ranking function. Now is you examine those assumptions carefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why update has zero probability and how do we fix it? So if you think about this from the moment you realize that that's because we have made an assumption that every query word must be drawn from the document in the user's mind. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document. So that's the improved model. An improvement here is to say that, well instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model. And so I show a model here. And we assume that this document is generated using this unigram language model. Now, this model doesn't necessarily assign zero probability for update in fact, we can assume this model does not assign zero probability for any word. Now if we're thinking this way then the generation process is a little bit different. Now the user has this model in mind instead of this particular document. Although the model has to be estimated based on the document. So the user can again generate the query using a singular process. Namely, pick a word for example, presidential and another word campaign.
7:29:Now the difference is that this time we can also pick a word like update, even though update doesn't occur in the document to potentially generate a query word like update. So that a query was updated 1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when our thinking of what the user is looking for in a more general way, that is unique language model instead of fixed document. So how do we compute this query likelihood? If we make this sum wide involved two steps. The first one is compute this model, and we call it document language model here. For example, I've shown two pulse models here, it's major based on two documents. And then given a query like a data mining algorithms the thinking is that we'll just compute the likelihood of this query. And by making independence assumptions we could then have this probability as a product of the probability of each query word. We do this for both documents, and then we can score these two documents and then rank them.
8:37:So that's the basic idea of this query likelihood retrieval function. So more generally this ranking function would look like in the following. Here we assume that the query has n words, w1 through wn, and then the scoring function. The ranking function is the probability that we observe this query, given that the user is thinking of this document. And this is assume it will be product of probabilities of all individual words. This is based on independent assumption. Now we actually often score the document before this query by using log of the query likelihood as shown on the second line.
9:26:Now we do this to avoid having a lot of small probabilities, mean multiply together. And this could cause under flow and we might loose the precision by transforming the value in our algorithm function. We maintain the order of these documents yet we can avoid the under flow problem. And so if we take longer than transformation of course, the product would become a sum as you on the second line here. So the sum of all the query words inside of the sum that is one of the probability of this word given by the document.
10:09:And then we can further rewrite the sum to a different form.
10:14:So in the first sum here, in this sum,
10:21:we have it over all the query words and query word. And in this sum we have a sum of all the possible words. But we put a counter here of each word in the query. Essentially we are only considering the words in the query, because if a word is not in the query, the count will be 0. So we're still considering only these n words. But we're using a different form as if we were going to take a sample of all the words in the vocabulary.
10:52:And of course, a word might occur multiple times in the query. That's why we have a count here.
11:00:And then this part is log of the probability of the word, given by the document language model.
11:08:So you can see in this retrieval function, we actually know the count of the word in the query. So the only thing that we don't know is this document language model.
11:17:Therefore, we have converted the retrieval problem include the problem of estimating this document language model.
11:25:So that we can compute the probability of each query word given by this document.
11:32:And different estimation methods would lead to different ranking functions. This is just like a different way to place document in the vector space which leads to a different ranking function in the vector space model. Here different ways to estimate will lead to a different ranking function for query likelihood. [MUSIC]
0:00:[SOUND] This lecture is about smoothing of language models.
0:11:In this lecture, we're going to continue talking about the probabilistic retrieval model. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method.
0:23:So you have seen this slide from a previous lecture. This is the ranking function based on the query likelihood.
0:32:Here, we assume that the independence of generating each query word And the formula would look like the following where we take a sum of all the query words. And inside the sum there is a log of probability of a word given by the document or document image model. So the main task now is to estimate this document language model as we said before different methods for estimating this model would lead to different retrieval functions. So in this lecture, we're going to be looking to this in more detail. So how do we estimate this language model? Well the obvious choice would be the maximum likelihood estimate that we have seen before. And that is we're going to normalize the word frequencies in the document.
1:24:And estimate the probability it would look like this.
1:30:This is a step function here.
1:35:Which means all of the words that have the same frequency count will have identical problem with it. This is another freedom to count, that has different probability. Note that for words that have not occurred in the document here
1:52:they will have 0 probability. So we know this is just like the model that we assume earlier in the lecture. Where we assume that the use of the simple word from the document to a formula to clear it.
2:09:And there's no chance of assembling any word that's not in the document and we know that's not good.
2:15:So how do we improve this? Well in order to assign a none 0 probability to words that have not been observed in the document, we would have to take away some probability mass from the words that are observed in the document. So for example here, we have to take away some probability of the mass because we need some extra probability mass for the words otherwise they won't sum to 1. So all these probabilities must sum to 1. So to make this transformation and to improve the maximum likelihood estimated by assigning non zero probabilities to words that are not observed in the data.
3:01:We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author
3:13:had been asking to write more words for the document, the author might have written other words. If you think about this factor then the a smoothed language model would be a more accurate than the representation of the actual topic. Imagine you have seen an abstract of a research article. Let's say this document is abstract.
3:39:If we assume and see words in this abstract that we have a probability of 0. That would mean there's no chance of sampling a word outside the abstract of the formulated query. But imagine a user who is interested in the topic of this subject. The user might actually choose a word that's not in that chapter to use as query. So obviously, if we has asked this author to write more author would have written a full text of the article. So smoothing of the language model is an attempt to try to recover the model for the whole article. And then of course, we don't have knowledge about any words that are not observed in the abstract. So that's why smoothing is actually a tricky problem. So let's talk a little more about how to smooth a language model. The key question here is, what probability should be assigned to those unseen words?
4:50:And there are many different ways of doing that.
4:53:One idea here, that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by a reference language model. That means if you don't observe the word in the dataset. We're going to assume that its probability is kind of governed by another reference language model that we will construct. It will tell us which unseen words would have a higher probability.
5:22:In the case of retrieval, a natural choice would be to take the collection language model as the reference language model. That is to say, if you don't observe a word in the document, we're going to assume that the probability of this word would be proportional to the probability of word in the whole collection. So more formally, we'll be estimating the probability of a word key document as follows.
5:48:If the word is seen in the document then the probability would be this counted the maximum likelihood estimate P sub c here. Otherwise, if the word is not seen in the document we're going to let probability be proportional to the probability of the word in the collection. And here the coefficient that offer is to control the amount of probability mass that we assign to unseen words.
6:22:Obviously, all these probabilities must sum to 1, so alpha sub d is constrained in some way.
6:29:So what if we plug in this smoothing formula into our query likelihood ranking function? This is what we will get.
6:37:In this formula, we have this as a sum over all the query words and those that we have written here as the sum of all the vocabulary, you see here. This is the sum of all the words in the vocabulary, but not that we have a count of the word in the query. So in fact, we are just taking a sample of query words. This is now a common way that we would use, because of its convenience in some transformations.
7:18:So this is as I said, this is sum of all the query words.
7:23:In our smoothing method, we assume that the words that are not observed in the method would have a somewhat different form of probability. Name it's four, this foru. So we're going to do then, decompose the sum into two parts.
7:38:One sum is over all the query words that are matching the document. That means that in this sum, all the words have a non zero probability in the document. Sorry, it's the non zero count of the word in the document. They all occur in the document.
8:02:And they also have to of course have a non zero count in the query. So these are the query words that are matching the document. On the other hand, in this sum we are taking a sum of all the words that are not all query was not matching the document.
8:25:So they occur in the query due to this term, but they don't occur in the document. In this case, these words have this probability because of our assumption about the smoothing. That here, these seen words have a different probability.
8:47:Now, we can go further by rewriting the second sum
8:52:as a difference of two other sums. Basically, the first sum is the sum of all the query words.
9:00:Now, we know that the original sum is not over all the query words. This is over all the query words that are not matched in the document.
9:12:So here we pretend that they are actually over all the query words. So we take a sum over all the query words. Obviously, this sum has extra terms that are not in this sum.
9:30:Because, here we're taking sum over all the query words. There, it's not matched in the document. So in order to make them equal, we will have to then subtract another sum here. And this is the sum over all the query words that are matching in the document.
9:51:And this makes sense, because here we are considering all query words. And then we subtract the query that was matched in the document. That would give us the query that was not matched in the document.
10:05:And this is almost a reverse process of the first step here.
10:12:And you might wonder why do we want to do that. Well, that's because if we do this, then we have different forms of terms inside of these sums. So now, you can see in this sum we have all the words matched, the query was matching the document with this kind of term.
10:36:Here we have another sum over the same set of terms, matched query terms in document. But inside the sum, it's different.
10:49:But these two sums can clearly be merged.
10:54:So if we do that, we'll get another form of the formula that looks like before me at the bottom here.
11:04:And note that this is a very interesting formula. Because here we combine these two that all or some of the query words matching in the document in the one sum here.
11:19:And the other sum now is decomposing into two parts. And these two parts look much simpler just, because these are the probabilities of unseen words.
11:31:This formula is very interesting because you can see the sum is now over
11:37:the match the query terms.
11:41:And just like in the vector space model, we take a sum
11:46:of terms that are in the intersection of query vector and the document vector.
11:51:So it already looks a little bit like the vector space model. In fact, there's even more similarity here as we explain on this slide. [MUSIC]
0:00:[SOUND]
0:12:So I showed you how we rewrite the query like holder which is a function into a form that looks like the formula of this slide after if we make the assumption about the smoothing, the language model based on the collection language model. Now if you look at this rewriting, it will actually give us two benefits. The first benefit is it helps us better understand this ranking function. In particular, we're going to show that from this formula we can see smoothing with the collection language model would give us something like a TF-IDF weighting and length normalization. The second benefit is that it also allows us to compute the query like holder more efficiently. In particular we see that the main part of the formula is a sum over the match of the query terms.
1:09:So this is much better than if we take a sum over all the words. After we smooth the document the damage model we essentially have non zero problem for all the words. So this new form of the formula is much easier to score or to compute.
1:27:It's also interesting to note that the last term here is actually independent of the document. Since our goal is to rank the documents for the same query we can ignore this term for ranking. Because it's going to be the same for all the documents. Ignoring it wouldn't affect the order of the documents.
1:49:Inside the sum, we
1:52:also see that each matched query term would contribute a weight.
1:58:And this weight actually is very interesting because it looks like a TF-IDF weighting. First we can already see it has a frequency of the word in a query just like in the vector space model. When we take a thought product, we see the word frequency in the query to show up in such a sum.
2:22:And so naturally this part would correspond between the vector element from the documented vector. And here indeed we can see it actually
2:35:encodes a weight that has similar in factor to TF-IDF weight.
2:41:I'll let you examine it, can you see it? Can you see which part is capturing TF? And which part is a capturing IDF weighting?
2:51:So if want you can pause the video to think more about it.
2:55:So have you noticed that this P sub seen is related to the term frequency in the sense that if a word occurs very frequently in the document, then the s made through probability here will tend to be larger. So this means this term is really doing something like a TF weight. Now have you also noticed that this term in the denominator is actually achieving the factor of IDF? Why, because this is the popularity of the term in a collection.
3:31:But it's in the denominator, so if the probability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and this is precisely what IDF weighting is doing.
3:47:Only that we now have a different form of TF and IDF.
3:51:Remember IDF has a logarithm of documented frequency. But here we have something different.
3:58:But intuitively it achieves a similar effect. Interestingly, we also have something related to the length of libation.
4:07:Again, can you see which factor is related to the document length in this formula?
4:14:What I just say is that this term is related to IDF weighting.
4:19:This collection probability, but it turns out that this term here is actually related to document length normalization. In particular, F of sub d might be related to document length. So it encodes how much probability mass we want to give to unseen worlds.
4:41:How much smoothing do we want to do? Intuitively, if a document is long, then we need to do less smoothing because we can assume that data is large enough. We probably have observed all the words that the author could have written. But if the document is short then r of sub t could be expected to be large. We need to do more smoothing. It's likey there are words that have not been written yet by the author. So this term appears to paralyze the non document in that other sub D would tend to be longer than or larger than for a long document. But note that alpha sub d also occurs here and so this may not actually be necessary paralyzing long documents. The effect is not so clear yet.
5:31:But as we will see later, when we consider some specific smoothing methods, it turns out that they do paralyze long documents. Just like in TF-IDF weighting and document length normalization formula in the vector space model.
5:47:So, that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing. We just need to assume that if we smooth with this collection memory model, then we would have a formula that looks like TF-IDF weighting and documents length violation.
6:08:What's also interesting that we have very fixed form of the ranking function.
6:14:And see we have not heuristically put a logarithm here.
6:19:In fact, you can think about why we would have a logarithm here. You look at the assumptions that we have made, it would be clear it's because we have used a logarithm of query like for scoring. And we turned the product into a sum of logarithm of probability, and that's why we have this logarithm.
6:40:Note that if only want to heuristically implement a TF weighting and IDF weighting, we don't necessary have to have a logarithm here. Imagine if we drop this logarithm, we would still have TF and IDF weighting.
6:55:But what's nice with problem risk modeling is that we are automatically given the logarithm function here. And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case if you try to drop the logarithm the model probably won't work as well as if you keep the logarithm.
7:19:So a nice property of problem risk modeling is that by following some assumptions and the probability rules we'll get a formula automatically. And the formula would have a particular form like in this case.
7:34:And if we heuristically design the formula we may not necessarily end up having such a specific formula.
7:41:So to summarize, we talked about the need for smoothing the document imaging model. Otherwise it would give zero probability for unseen words in the document, and that's not good for storing a query with such an unseen word.
7:59:It's also necessary, in general, to improve the accuracy of estimating the model represent the topic of this document. The general idea of smoothing in retrieval is to use the connecting memory model to,
8:17:to give us some clue about which unseen words should have a higher probability. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection.
8:29:With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has effect of TF-IDF weighting and document length normalization. We also see that, through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on matched query terms, just like in the vector space model. But, the actual ranking function is given us automatically by the probability rules and assumptions that we have made. And like in the vector space model where we have to heuristically think about the form of the function. However, we still need to address the question how exactly we should smooth the document and the model. How exactly we should use the reference and model based on the connection to adjust the probability of the maximum micro is made of and this is the topic of the next batch. [MUSIC]
0:00:[SOUND]
0:07:This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model.
0:16:In this lecture, we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method. And we're going to talk about specifically the smoothing methods used for such a retrieval function.
0:33:So this is a slide from a previous lecture where we show that with a query likelihood ranking and smoothing with the collection language model, we add up having a retrieval function that looks like the following. So this is the retrieval function based on these assumptions that we have discussed. You can see it's a sum of all the matching query terms, here. And inside its sum is the count of the term in the query and some weight for the term in the document.
1:12:We have t of i, the f weight here, and then we have another constant here in n.
1:20:So clearly if we want to implement this function using programming language, we still need to figure out a few variables. In particular, we're going to need to know how to estimate the probability of a word exactly and how do we set alpha.
1:40:So in order to answer this question, we have to think about very specific smoothing methods, and that is main topic of this lecture.
1:48:We're going to talk about two smoothing methods. The first is simple linear interpolation with a fixed coefficient. And this is also called a Jelinek-Mercer smoothing.
2:01:So the idea is actually very simple. This picture shows how we estimate a document language model by using maximum likelihood estimate. That gives us word counts normalized by the total number of words in the text. The idea of using this method
2:22:is to maximize the probability of the observed text. As a result, if a word like network is not observed in the text, it's going to get 0 probability, as shown here.
2:37:So the idea of smoothing, then, is to rely on collection language model where this word is not going to have a zero probability to help us decide what nonzero probability should be assigned to such a word. So we can note that network has a nonzero probability here. So in this approach what we do is we do a linear interpolation between the maximum likelihood placement here and the collection language model, and this is computed by the smoothing parameter lambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is, the more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzero probabilities to a word like network. So let's see how it works for some of the words here.
3:32:For example, if we compute the smooth probability for text.
3:37:Now the maximum likelihood estimated gives us 10 over 100, and that's going to be here.
3:44:But the collection probability is this. So we'll just combine them together with this simple formula.
3:53:We can also see the word network, which used